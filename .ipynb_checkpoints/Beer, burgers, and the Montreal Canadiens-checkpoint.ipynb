{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "Like many of you, whenever the Montreal Canadiens hockey team have a match, it can happen that my compadres and I might find ourselves in a sports bar. \n",
    " \n",
    "Guzzling down pints and stuffing my face face with burgers among my fellow hockey fans, I sometimes wonder of the relationship between our beloved sports team’s performance on ice and how it affects our performance with the pints and burgers (i.e how much revenue sport bars make). \n",
    " \n",
    "Have you thought about that too? Of course you have.  Because you are of the numerically inclined! Which is why the meanderings of your life have brought you here to my humble page. \n",
    "\n",
    "So let's see if how the Montreal Canadiens are doing determine how well sports bars are doing.\n",
    "\n",
    "About the data:\n",
    " \n",
    "La cage aux sport, owned by Sportscene is one of the biggest sport bars in Quebec and only operates in Quebec. I decided to collect their quarterly revenues from their financial statements which I found on http://www.sedar.com\n",
    " \n",
    "As for the Hockey data, I got it from http://ourhistory.canadiens.com/stats/search#/dashboard/players/\n",
    " \n",
    "I will first take a look at the sport bar data alone and see what I can make of it. Then I will bring in the hockey data and see if it adds anything.\n",
    "\n",
    "So let’s take a look! \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "At first I'm just going to load up the data and wrangle it a little bit in order to to get started. You can look at it or hold your nose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'sportscenedata/sportscene.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-cf9f14181ac4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmyscripts\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sportscenedata/sportscene.csv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"date\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"quarter\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"half\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"year\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alexkeenan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    644\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    645\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 646\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alexkeenan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alexkeenan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    728\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 730\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    731\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    732\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alexkeenan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    921\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    922\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 923\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    924\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    925\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alexkeenan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1388\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1390\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1392\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas\\parser.c:4184)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas\\parser.c:8449)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'sportscenedata/sportscene.csv' does not exist"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import warnings\n",
    "import itertools\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#plt.style.use(\"seaborn\")\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15, 6)\n",
    "from myscripts import *\n",
    "\n",
    "data=pd.read_csv(\"sportscene.csv\",index_col=\"date\",parse_dates=True)\n",
    "\n",
    "data.columns=[\"quarter\",\"half\",\"year\"]\n",
    "data.sort_index(inplace=True)\n",
    "\n",
    "data=data.groupby(data.index).first()\n",
    "\n",
    "data=data.iloc[:,[0,2]]\n",
    "data.iloc[:,0]=data.iloc[:,0].str.replace(\",\",\"\")\n",
    "\n",
    "data=data.apply(pd.to_numeric)\n",
    "data=data.fillna(0)\n",
    "data[\"revenue\"]=data.iloc[:,0]+data.iloc[:,1]\n",
    "\n",
    "yearly_data=data.revenue[data.index.month==8]\n",
    "quarters_data=data.quarter[data.index.month!=8]\n",
    "\n",
    "if quarters_data.shape[0]%3!=0:\n",
    "    quarters_data=quarters_data.iloc[:-(quarters_data.shape[0]%3)] #not done with current year,\n",
    "\n",
    "\n",
    "grouped_up=np.split(quarters_data, quarters_data.shape[0]/3)\n",
    "\n",
    "totals=[]\n",
    "for tmp in grouped_up:\n",
    "    totals.append(tmp.sum())\n",
    "    \n",
    "#Every 4rth bar revenue data point shows the revenue for the entire year, so to find how that quarter did, need to difference with the rest\n",
    "\n",
    "data.revenue[data.index.month==8]=data.revenue[data.index.month==8].values-totals\n",
    "\n",
    "#resampling to MONTHLY so as to get the end of month\n",
    "data=data.revenue.resample(\"M\").sum()\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "all_dates_inplay=data.index.date\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "There! Now let's see what sportscene revenue looks like on a quarterly basis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Pretty choppy eh? We'll get a clearer look as to what's behind this real soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Creating stats for bar data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "window=8\n",
    "bar_data_stats=create_stats(pd.DataFrame(data),window)\n",
    "bar_data=pd.concat([data,bar_data_stats],axis=1)\n",
    "bar_data.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "What does my custom function do? It create creates statistics based on the inputed data. \n",
    "\n",
    "The window parameter is used for the rolling average calculated within. A window of 8 means it will calculated mean will be of the 8 past data points. 8 quarters means two years. \n",
    "\n",
    "To give you a quick idea, here's what they're called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(bar_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "nber_of_graphs=bar_data.columns.shape[0]\n",
    "graphs_per_row=2\n",
    "nber_of_rows= np.ceil(nber_of_graphs/graphs_per_row)\n",
    "\n",
    "nber_of_rows_number=nber_of_rows*100\n",
    "graphs_per_row_number=graphs_per_row*10\n",
    "\n",
    "i=1\n",
    "base_number=nber_of_rows_number+graphs_per_row_number\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (15, 3*nber_of_rows)\n",
    "#['revenueDifferencedRollingStandardization','revenueRollingNormalization','revenueDifferencedRollingNormalization']:\n",
    "    \n",
    "for each_column in bar_data.columns:    \n",
    "\n",
    "    plt.subplot(base_number+i) \n",
    "    plt.plot(bar_data[each_column])\n",
    "    plt.title(each_column)\n",
    "    i+=1\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Above you can see what the extra columns look like. They're just variations of the original revenue input. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (15, 9)\n",
    "\n",
    "chosen_bar_data=bar_data.revenueRelAvg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "chosen_bar_data.groupby(chosen_bar_data.index.month).mean().plot(kind=\"bar\",title=\"Quarterly change relative to %.2f year average\"%(window/4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "So this is a little better. Here we can clearly see that typically, it's third quarter is its weakest with it's quarterly revenue typically being below its 2 year quarterly revenue. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now this data across time is what's called a timeseries. The key thing to remember is that you can't just run a regression on it straight off the bat. Regressions along with many other statistical tests assume the data points are independent of each other and that the probability distribution of the data and its variance remain the same throughout (stationarity). \n",
    "\n",
    "In this case they aren't independent as they all depend on time. Also the quarterly revenue seems to increase gradually while never reverting to a mean. In theory it could go on to infinity, which makes this definitely non-stationary.\n",
    "\n",
    "To make the series stationary, we simply need to difference it. Now we are looking at the CHANGE in quarterly revenue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#log them to help with the stationarity\n",
    "#stationary_data=np.log1p(bar_data[[0]]-bar_data[[0]].shift())\n",
    "\n",
    "stationary_data=chosen_bar_data-chosen_bar_data.shift()\n",
    "#stationary_data=chosen_bar_data\n",
    "\n",
    "stationary_data.dropna(inplace=True)\n",
    "stationary_data.plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Better! Let's test it for stationarity to make sure. The best method for detecting stationarity is with what's called the  the Dickey-Fuller test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_stationarity(stationary_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Using the Dickey-Fuller test we can see that our test statistic is even lower than that of the 1% critical value, meaning there's less than a 1% chance that we would've gotten these results and that the series was not stationary. \n",
    "\n",
    "\n",
    "Moving on, let's look to see if our data has any autocorrelation, which is to say, do previous results help predict future results? There are quick ways to tell if there might be some."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lag_plot(pd.DataFrame(stationary_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This chart plots every data point along with its next data point as the coordinate for each dot. We can see that the data is not totally random. The datapoints seem to avoid the bottom left and top right. This indicates that there might be some structure to these data points (i.e presence of autocorrelations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "time_series_info_on_y(stationary_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The first two graphs are very similar as in they only look at the autocorrelation between the current value and it's lags. \n",
    "\n",
    "The Autocorrelation Function (ACF) basically incorporates all the lags un until the selected value. So ACF(4) looks at how all the lags up until lag 4 correlate with your prediction\n",
    "\n",
    "Partial Autocorrelation Function (PACF) only looks at that one particular lag. Notice here that it looks like there is a very strong negative correlation with the first lag, then another one for the third lag (albeit less strong).\n",
    "\n",
    "Anything outside of the shaded area usually implies some statistically significant relationship. Anything within the shaded area is considered noise. So the lag of 2 alone PACF(2) doesn't correlate with your prediction.\n",
    "\n",
    "I can also tell from the ACF(2) that the strong correlation found is explained by the strong relationship for lag 1 found in the PACF(1). The lag of 2 doesn't add anything to the mix, which explains why when lag 1 + lag 2 are binned together the strength of the autocorrelation is less than lag 1 alone. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Game plan\n",
    "\n",
    "So there seems to be some autocorrelation which again means that past values help explain future values. \n",
    "I will first try to predict the next quarterly revenue purely based on its past revenue. Then I will try to incorporate hockey data and see if that improves things.\n",
    "\n",
    "I will be using what's called a SARIMAX model and then I will try a machine learning technique called the ElasticNet regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    " \n",
    "# Going the SARIMAX route below. No exogenous variables, just pure lags\n",
    "\n",
    "Before tackling SARIMAX, what is an ARIMA model? ARIMA stands for AutoRegressive Integrated Moving Average.\n",
    "It allows you to take in raw timeseries apply transformations to make it stationary, analyze and then come up with a prediction. \n",
    "\n",
    "The variables it takes in are ARIMA(p,d,q). \n",
    "p stands for how many lagged values do you want to take into account when making your prediction for the next value.\n",
    "This speaks directly to the AutoRegressive component of the model. \n",
    "For example a lag of 1 (AR(1)) means you are using today's temperature to predict tomorrow's temperature. \n",
    "AR(2) means tomorrow's tempurature is correlated with yesterday and today's temperature.\n",
    "\n",
    "d is for differencing. Which explains the Integrated part of the arIma model. The number d just means how many times you differenced your data. So a difference of 1 means Y2-Y1, Y3-Y2. So basically instead of having the temperature of everyday, you have the difference in temperature compared to the previous day. If d=2, then you'd be dealing with the difference of the difference in temperatures.\n",
    "\n",
    "q is for moving average (MA). The key thing to keep in mind here is that we are looking at the ERRORS for \"q\" lags and trying to see if they correlate with our prediction. What are those errors you might ask?\n",
    "\n",
    "When you do an AR(2) process, we try to predict using 2 lags. Naturally when we try to fit these with lots of data, you'll never get an exact fit. You'll get errors for each lags.It's the noise, the residual. Those errors in our predictions is what we'll be using for MA(2).\n",
    "If the noise is completely random then it's uninformative. But in some cases there will still be some structure or information left in them, which why we might have to take them into account in order to better predict.\n",
    "\n",
    "And finally we have SARIMAX. \n",
    "\n",
    "It's simply an ARIMA model but with seasonality in mind. If you run a business you typically have high and low seasons. And so if I want to predict my sales for the summer, the best predictor for it might simply be last year's summer, but not so much all the time/lags in between. The model has extra parameters P,D,Q which do the same things as p,d,q but compared across their matching seasons.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Define the p, d and q parameters\n",
    "\n",
    "Now there's a whole lot of litterature on how to choose your parameters. \n",
    "View this link here for a taste.\n",
    "https://people.duke.edu/~rnau/411sdif.htm\n",
    "The thing is it can take a lot of time to do it manually. So what I've done is simply used gridsearch that will look at all possible combinations and try them all. The best model survives.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Below I split my data into train and test sets. I decided not to use those from sklearn preprocessing because from what I've read it randomizes the datapoints before assigning what's for training and what's for testing. This would be great were our datapoints independent of each other. In our case with timeseries, they are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#make sure to also check window\n",
    "train_pct=0.8\n",
    "split_data=train_test(train_pct,y=bar_data)\n",
    "\n",
    "y_train=split_data[\"y_train\"]\n",
    "y_test=split_data[\"y_test\"]\n",
    "mid_data_index=split_data[\"mid_data_index\"]\n",
    "y_all=bar_data\n",
    "train_test_index=split_data[\"train_test_index\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The following code was based on this terrific guide right here https://www.digitalocean.com/community/tutorials/a-guide-to-time-series-forecasting-with-arima-in-python-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "p=q = range(0,4)\n",
    "\n",
    "d=range(0,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Generating all different combinations of p, q and q triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pdq = list(itertools.product(p, d, q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Generate all different combinations of seasonal p, q and q triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "seasonal_pdq = [(x[0], x[1], x[2], 4) for x in list(itertools.product(p, d, q))]\n",
    "\n",
    "print('Examples of parameter combinations for Seasonal ARIMA...')\n",
    "print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You can set do_sarimax=True if you want to run the grid search, but it could take a while. By default I set it to the answer the gridsearch would have provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "do_sarimax=False #in case I don't want to run this thing again.\n",
    "\n",
    "if do_sarimax==True:\n",
    "    warnings.filterwarnings(\"ignore\") # specify to ignore warning messages\n",
    "    best_score, best_cfg = float(\"inf\"), None\n",
    "    for each_measure_unit in ['revenue','revenueRollingMean','revenueRelAvg','revenueRollingStandardization','revenueRollingNormalization']:\n",
    "\n",
    "        for param in pdq:\n",
    "            for param_seasonal in seasonal_pdq:\n",
    "\n",
    "                try:\n",
    "                    mod = sm.tsa.statespace.SARIMAX(y_train[each_measure_unit],\n",
    "                                                    order=param,\n",
    "                                                    exog=None,\n",
    "                                                    seasonal_order=param_seasonal,\n",
    "                                                    enforce_stationarity=False,\n",
    "                                                    enforce_invertibility=False)\n",
    "\n",
    "                    results = mod.fit()\n",
    "\n",
    "\n",
    "                    if results.aic< best_score:\n",
    "                        best_score=results.aic\n",
    "                        best_cfg=[param, param_seasonal]\n",
    "                        best_measure_unit=each_measure_unit\n",
    "                        print('ARIMA{}x{}4 - AIC:{} with data {}'.format(param, param_seasonal, results.aic,each_measure_unit))\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "    print(\"BEST FIT\")            \n",
    "    print('ARIMA{}x{}4 - AIC:{} with data {}'.format(best_cfg[0], best_cfg[1], best_score,best_measure_unit))\n",
    "else:\n",
    "    print(\"skipping Sarimax\")\n",
    "    #ARIMA(2, 0, 1)x(0, 0, 0, 4)4 - AIC:-90.93117578195475 with data revenueRelAvg\n",
    "    best_measure_unit=\"revenueRelAvg\"\n",
    "    best_cfg=[(2, 0, 1), (0, 0, 0, 4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We're judging the model with the Akaike Information Criterion (AIC score)\n",
    "\n",
    "I won't be discussing this at length but the key thing to remember about AIC is that it balances complexity and accuracy. Adding more parameters or lags will can always make the model more precise. But does that extra parameter REALLY add enough to warrant adding that parameter in the first place? Is adding more complexity to the model worth that extra bit of accuracy? The AIC score will tell us. \n",
    "The better the model the lower the AIC score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finally it seem that model is best at predicting the relative average rolling mean (which is how the quarterly revenue compares to its two year average) which is very informative still.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# NOW TO ANALYZE THE BEST ONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "mod = sm.tsa.statespace.SARIMAX(y_all[best_measure_unit],\n",
    "                                order=best_cfg[0],#order=(1, 0, 1),\n",
    "                                seasonal_order=best_cfg[1],#seasonal_order=(1, 0, 1, 12),\n",
    "                                enforce_stationarity=False,\n",
    "                                enforce_invertibility=False)\n",
    "\n",
    "results = mod.fit()\n",
    "\n",
    "print(results.summary().tables[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "So the best model to survive in this case used 1 lag. Curiously enough there's no differencing and nothing with seasonality was done. The fact that nothing was done with seasonality can be explained away since there are only 4 periods per year. In this case it looks like we're using 1 lag with a negative coefficient, which is all you need to explain the oscillation between the quarterly revenues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Statsmodel the library used here has great charts that are automatically produced for you whenever you run your Sarimax models.\n",
    "results.plot_diagnostics(figsize=(15, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The top left, top right and bottom left charts basically plot the errors. We want to make sure that the errors are look somewhat normal (i.e follows a gaussian distribution). From the look of things, it's not a perfect Gaussian, looking at the top right you can see that too many of the results are positive. \n",
    "Still it's not too far off. Let's go with it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# FORECASTING\n",
    "Let's see what our predictions would look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "pred = results.get_prediction(start=pd.to_datetime(y_test.index[0]), dynamic=False)\n",
    "#The dynamic=False argument ensures that we produce one-step ahead forecasts, meaning that forecasts at each point are generated using the full history up to that point.\n",
    "pred_ci = pred.conf_int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "ax = y_all[best_measure_unit][mid_data_index:].plot(label='observed')\n",
    "pred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7)\n",
    "\n",
    "ax.fill_between(pred_ci.index,\n",
    "                pred_ci.iloc[:, 0],\n",
    "                pred_ci.iloc[:, 1], color='k', alpha=.2)\n",
    "\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Bar Revenue')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "\n",
    "# Computing the mean square error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_forecasted = pred.predicted_mean \n",
    "y_truth = y_test[best_measure_unit]\n",
    "mse = ((y_forecasted - y_truth) ** 2).mean()\n",
    "print('The Mean Squared Error of our forecasts is {}'.format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This doesn't look too bad, but the main thing to keep in mind here is that our model is constantly being updated with the real observations that took place. \n",
    "\n",
    "Often times the TRUE test of a good model is how much it can predict when it's fed its own predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pred_dynamic = results.get_prediction(start=pd.to_datetime(y_test.index[0]), dynamic=True, full_results=True)\n",
    "pred_dynamic_ci = pred_dynamic.conf_int()\n",
    "\n",
    "ax = chosen_bar_data[mid_data_index:].plot(label='observed', figsize=(20, 15))\n",
    "pred_dynamic.predicted_mean.plot(label='Dynamic Forecast', ax=ax)\n",
    "\n",
    "ax.fill_between(pred_dynamic_ci.index,\n",
    "                pred_dynamic_ci.iloc[:, 0],\n",
    "                pred_dynamic_ci.iloc[:, 1], color='k', alpha=.25)\n",
    "\n",
    "ax.fill_betweenx(ax.get_ylim(), pd.to_datetime(mid_data_index), chosen_bar_data.index[-1],\n",
    "                 alpha=.1, zorder=-1)\n",
    "\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Bar performance')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_forecasted = pred_dynamic.predicted_mean \n",
    "y_truth = y_test[best_measure_unit]\n",
    "mse = ((y_forecasted - y_truth) ** 2).mean()\n",
    "print('The Mean Squared Error of our forecasts is {}'.format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Not very good eh? Basically we can get some idea of where we'll be for the NEXT period. But anything farther than that and the model has no idea. \n",
    "\n",
    "Look how big the confidence intervals are. As time goes on, the confidence interval increases since there are more places the real observations could be as it has more time to move.\n",
    "\n",
    "\n",
    "This configuration is entirely dependent on the previous lag. Which when predicted on its own prediction makes a for a pretty stable answer...\n",
    "\n",
    "\n",
    "Now so far we've merely tried to predict the companies business using it's own past sales. Let's see how well it fares when we implement some hockey data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Hockey data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now let's take a look at the hockey data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hockey_data=pd.read_csv('mtl_hockey_granular.csv',usecols=[\"DATE\",\"AWAY\",\"HOME\"],index_col=\"DATE\",parse_dates=True)\n",
    "hockey_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Just looking at hockey games, what are a few of its characteristics that could determine how much business sports bars get? \n",
    "Is the team playing at home or away? \n",
    "(If at home, they might just go to the stadium instead of the bar)\n",
    "Is the game on a Thursday-Saturday night? Or is it on a monday? \n",
    "Have the MTL Canadiens been losing for a while? Winning for a while? What about the number of goals? \n",
    "Is the variance of goals a big draw?\n",
    "\n",
    "Let's transform the table into a more useful format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hockey_data.columns=['Away','Home']\n",
    "hockey_data[\"MtlGoals\"]=hockey_data.Home\n",
    "hockey_data[\"OppGoals\"]=hockey_data.Home\n",
    "\n",
    "#separating the goals from the name\n",
    "away_goals=hockey_data.Away.str.extract('(\\d+)').astype(int)\n",
    "home_goals=hockey_data.Home.str.extract('(\\d+)').astype(int)\n",
    "\n",
    "#this version will cause the 'A value is trying to be set on a copy of a slice from a DataFrame.' problem\n",
    "#hockey_data[hockey_data.Away.str.contains(\"MTL\")].\"MtlGoals\"=away_goals[hockey_data.Away.str.contains(\"MTL\")].values\n",
    "\n",
    "mtl_home=hockey_data.Home.str.contains(\"MTL\")\n",
    "mtl_away=~hockey_data.Home.str.contains(\"MTL\")\n",
    "\n",
    "hockey_data.loc[mtl_away,\"MtlGoals\"]=away_goals[mtl_away].values\n",
    "hockey_data.loc[mtl_home,\"MtlGoals\"]=home_goals[mtl_home].values\n",
    "\n",
    "hockey_data.loc[mtl_home,\"OppGoals\"]=away_goals[mtl_home].values\n",
    "hockey_data.loc[mtl_away,\"OppGoals\"]=home_goals[mtl_away].values\n",
    "\n",
    "hockey_data.Away=hockey_data.Away.str.replace('\\d+', '')\n",
    "hockey_data.Home=hockey_data.Home.str.replace('\\d+', '')\n",
    "\n",
    "hockey_data[\"Opp\"]=hockey_data.Away\n",
    "hockey_data.loc[mtl_away,\"Opp\"]=hockey_data.Home[mtl_away].values\n",
    "hockey_data.loc[mtl_home,\"Opp\"]=hockey_data.Away[mtl_home].values\n",
    "\n",
    "#I need these to be numbers because later on I will be summing them up\n",
    "\n",
    "hockey_data.loc[mtl_away,\"Away\"]=1\n",
    "hockey_data.loc[mtl_away,\"Home\"]=0\n",
    "\n",
    "hockey_data.loc[mtl_home,\"Away\"]=0\n",
    "hockey_data.loc[mtl_home,\"Home\"]=1\n",
    "\n",
    "hockey_data[\"Win\"]=0\n",
    "hockey_data[\"Tie\"]=0\n",
    "hockey_data[\"Defeat\"]=0\n",
    "\n",
    "\n",
    "wins=hockey_data.MtlGoals>hockey_data.OppGoals\n",
    "ties=hockey_data.MtlGoals==hockey_data.OppGoals\n",
    "losses=hockey_data.MtlGoals<hockey_data.OppGoals\n",
    "\n",
    "hockey_data.loc[wins,\"Win\"]=1\n",
    "hockey_data.loc[ties,\"Tie\"]=1\n",
    "hockey_data.loc[losses,\"Defeat\"]=1\n",
    "\n",
    "#days of the week\n",
    "\n",
    "hockey_data[\"monday\"]=hockey_data.index.dayofweek==0\n",
    "hockey_data[\"tuesday\"]=hockey_data.index.dayofweek==1\n",
    "hockey_data[\"wednesday\"]=hockey_data.index.dayofweek==2\n",
    "hockey_data[\"thursday\"]=hockey_data.index.dayofweek==3\n",
    "hockey_data[\"friday\"]=hockey_data.index.dayofweek==4\n",
    "hockey_data[\"saturday\"]=hockey_data.index.dayofweek==5\n",
    "hockey_data[\"sunday\"]=hockey_data.index.dayofweek==6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hockey_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Much more useful no? We've turned Away and Home into columns holding either 1 or 0 (all from the perspective of the MTL Canadiens of course) and we've added a few more parameters as well. \n",
    "\n",
    "Let's make sure they're all interpreted as numbers (sometimes when you read them from datasources, they're interpreted as \"strings\" or \"objects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hockey_data.Away=pd.to_numeric(hockey_data.Away);\n",
    "hockey_data.Home=pd.to_numeric(hockey_data.Home);\n",
    "hockey_data.MtlGoals=pd.to_numeric(hockey_data.MtlGoals);\n",
    "hockey_data.OppGoals=pd.to_numeric(hockey_data.OppGoals);\n",
    "hockey_data=hockey_data.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For the sports bar data, the most granular I could get it was on a quarterly basis. \n",
    "This means that we need to sum up the hockey activity on a quarterly basis as well to see how one affects the other.\n",
    "The quarters in question are February, May, August, November "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "first_feb=hockey_data.index.month==2\n",
    "index_of_first_feb = np.where(first_feb==True)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The above looks a little hackey, but really it just finds the first occurence of February so that we can start there and then start summing up the data 3 months at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hockey_data=hockey_data.iloc[index_of_first_feb:,:]\n",
    "\n",
    "hockey_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "quarterly_hockey_data=hockey_data.resample(\"3M\").sum(); \n",
    "#turning NaN values into 0s. \n",
    "quarterly_hockey_data=quarterly_hockey_data.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "When we resample quarterly, the pandas dataframe will actually create any datapoints that are missing. In those cases it will produce a row with \"NaN\" so we turn those into 0s. This will be the case for the year 2005 where a hockey lockout occured.\n",
    "\n",
    "It's important that we don't get rid of these empty values because later on will be comparing this with the business data and those dates need to match up, whether hockey occured or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "And there you go, now we have something that matches the quarterly revenue data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "quarterly_hockey_data[-10:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# creating stats again...\n",
    "Just like before, we'll be creating statistics based on each one of these columns in the hopes that they can help us with the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#quarterly_hockey_data_extra=create_stats(quarterly_hockey_data[[\"MtlGoals\",\"OppGoals\",\"Win\",\"Tie\",\"Defeat\"]],window=window)\n",
    "quarterly_hockey_data_extra=create_stats(quarterly_hockey_data,window=window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "quarterly_hockey_data_with_stats=pd.concat([quarterly_hockey_data,quarterly_hockey_data_extra],axis=1)\n",
    "quarterly_hockey_data_with_stats.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Saving the data so far for future reference..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "quarterly_hockey_data_with_stats.to_csv(\"hockey_data_granular_refined.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "And now we merge the hockey dataset with the sports bar dataset. The following bit of code ensure we deal only with the timespans the hockey and the sports bar dataset overlap on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "dates_in_common=np.intersect1d(quarterly_hockey_data_with_stats.index.values, bar_data.index.values)\n",
    "\n",
    "smaller_hockey_df=quarterly_hockey_data_with_stats.loc[dates_in_common,:]\n",
    "smaller_business_data=bar_data.loc[dates_in_common]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "merged_df=pd.concat([smaller_hockey_df,smaller_business_data],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's take a quick look at any correlations. While correlation doesn't mean causation, it could give us some clues as to what possible relationships might exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "sns.heatmap(merged_df.corr().abs())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's zoom in on the part we're interested in. Correlation with revenue or any of its variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "correlations_of_interest=merged_df.corr().abs().iloc[-8:,:-8]\n",
    "\n",
    "sns.heatmap(correlations_of_interest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Still too messy. Let's just look at the top candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "min_corr=0.5\n",
    "\n",
    "dict_of_df={}\n",
    "for each_index in correlations_of_interest.index:\n",
    "    cols_of_interest=correlations_of_interest.loc[each_index,:].values>0.5\n",
    "    if correlations_of_interest.loc[each_index,cols_of_interest].shape[0]>0:\n",
    "        \n",
    "        dict_of_df[each_index]=pd.DataFrame()\n",
    "        dict_of_df[each_index]=correlations_of_interest.loc[each_index,cols_of_interest]\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for each_key in dict_of_df.keys():\n",
    "    print(each_key)\n",
    "    print(dict_of_df[each_key].drop_duplicates().sort_values(ascending=False)[:10])\n",
    "    dict_of_df[each_key].sort_values(ascending=False)[:10].plot(label=each_key,kind=\"bar\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here we can see the correlations of interest and some of it is surprising. Revenue correlating with the number of ties? Or the revenue Relative average rolling mean being correlated with the standard deviation of the number of games on saturdays? \n",
    "\n",
    "Clearly some of these correlations might be spurious. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# TRYING SARIMAX AGAIN, WITH BOTH HOCKEY AND BAR PERFORMANCE\n",
    "\n",
    "Now doing multiple regressions can be tricky. Given some variables it might detect only a few to be statistically significant, yet when you rerun the regression with only those pointed out as statistically significant, they statistical significance might disappear. Let me show what it looks like if we put all of hockey data in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define the p, d and q paradmeters to take any value between 0 and 2\n",
    "p = d = q = range(0, 2)\n",
    "\n",
    "# Generate all different combinations of p, q and q triplets\n",
    "pdq = list(itertools.product(p, d, q))\n",
    "\n",
    "# Generate all different combinations of seasonal p, q and q triplets\n",
    "seasonal_pdq = [(x[0], x[1], x[2], 4) for x in list(itertools.product(p, d, q))]\n",
    "\n",
    "print('Examples of parameter combinations for Seasonal ARIMA...')\n",
    "print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Like before you can set do_sarimax=True if you want to run the grid search, but it could take a while. By default I set it to the answer the gridsearch would have provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "do_sarimax=False\n",
    "x=smaller_hockey_df\n",
    "if do_sarimax==True: #Sarimax can be time consuming. If I already know the answer, don't want this to run again.\n",
    "    warnings.filterwarnings(\"ignore\") # specify to ignore warning messages\n",
    "    best_score, best_cfg = float(\"inf\"), None\n",
    "\n",
    "    \n",
    "\n",
    "    compare_matching_measure_only=False\n",
    "    for each_measure_unit in ['revenueRelAvg','revenueRollingStandardization','revenueRelAvgRollingMean','revenueRollingNormalization']:\n",
    "        print(\"Now checking out \"+each_measure_unit)\n",
    "\n",
    "        chosen_bar_data=smaller_business_data[each_measure_unit]\n",
    "        #COMPARE ONLY WITH MATCHING MEASURE TYPE FOR X\n",
    "        if compare_matching_measure_only==False:\n",
    "\n",
    "            measure_units=['RelAvg','RollingStandardization','RollingNormalization','RelAvgRollingMean']\n",
    "            chosen_measure_unit=[measure_unit for measure_unit in measure_units if measure_unit in each_measure_unit][0]\n",
    "\n",
    "            print(\"chosen_measure_unit is \"+chosen_measure_unit)\n",
    "            cols_I_want=[col for col in x.columns if chosen_measure_unit in col]\n",
    "            x_columns=x[cols_I_want]\n",
    "        else:\n",
    "            x_columns=x;\n",
    "\n",
    "        for param in pdq:\n",
    "            for param_seasonal in seasonal_pdq:\n",
    "\n",
    "                try:\n",
    "                    mod = sm.tsa.statespace.SARIMAX(chosen_bar_data,\n",
    "                                                    order=param,\n",
    "                                                    exog=x_columns,\n",
    "                                                    seasonal_order=param_seasonal,\n",
    "                                                    enforce_stationarity=False,\n",
    "                                                    enforce_invertibility=False)\n",
    "                    results = mod.fit()\n",
    "\n",
    "\n",
    "                    if results.aic< best_score:\n",
    "                        best_score=results.aic\n",
    "                        best_cfg=[param, param_seasonal]\n",
    "                        best_measure_unit=each_measure_unit.copy()\n",
    "                        best_x_columns=x_columns\n",
    "                        best_y_columns=chosen_bar_data\n",
    "\n",
    "                        print('ARIMA{}x{}4 - AIC:{} with data {}'.format(param, param_seasonal, results.aic,each_measure_unit))\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "    print(\"BEST FIT\")            \n",
    "    print('ARIMA{}x{}4 - AIC:{} with data {}'.format(best_cfg[0], best_cfg[1], best_score,best_measure_unit))\n",
    "else:\n",
    "    #ARIMA(1, 1, 0)x(0, 0, 0, 4)4 - AIC:-304.59500171522893 with data revenueRelAvg\n",
    "    best_measure_unit=\"revenueRelAvg\"\n",
    "    \n",
    "    cols_I_want=[col for col in x.columns if \"RelAvg\" in col]\n",
    "    x_columns=x[cols_I_want]\n",
    "    best_x_columns=x_columns\n",
    "    best_y_columns=smaller_business_data[best_measure_unit]\n",
    "    best_cfg=[(1, 1, 0), (0, 0, 0, 4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "chosen_bar_data=best_y_columns\n",
    "\n",
    "\n",
    "mod = sm.tsa.statespace.SARIMAX(chosen_bar_data,\n",
    "                                order=best_cfg[0],#order=(1, 0, 1),\n",
    "                                exog =best_x_columns, #putting in other regressors\n",
    "                                seasonal_order=best_cfg[1],#seasonal_order=(1, 0, 1, 12),\n",
    "                                enforce_stationarity=False,\n",
    "                                enforce_invertibility=False)\n",
    "\n",
    "results = mod.fit()\n",
    "\n",
    "print(results.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Some analysis...\n",
    "\n",
    "Having put in all hockey variables in one shot through the SARIMAX process, you'll notice that for a lot of them the P value is quite high. \n",
    "\n",
    "The P value shows you the odds of your parameter's coefficient being 0(meaning your parameter is useless) and still producing the results you have. Usually you're going to want a value of 0.05 or lower.\n",
    "\n",
    "(While a low P value might mean your parameter is statisticically significant it doesn't necessarily mean the parameter is important. But still at the very least, the parameter needs to be statistically significant before we can wonder if it's useful or not. )\n",
    "\n",
    "Still let's complete the rest of the analysis and see what it would have predicted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "results.plot_diagnostics(figsize=(15, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pred = results.get_prediction(start=pd.to_datetime(y_test.index[0]), dynamic=False)\n",
    "#The dynamic=False argument ensures that we produce one-step ahead forecasts, meaning that forecasts at each point are generated using the full history up to that point.\n",
    "pred_ci = pred.conf_int()\n",
    "\n",
    "ax = chosen_bar_data[mid_data_index:].plot(label='observed')\n",
    "pred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7)\n",
    "\n",
    "ax.fill_between(pred_ci.index,\n",
    "                pred_ci.iloc[:, 0],\n",
    "                pred_ci.iloc[:, 1], color='k', alpha=.2)\n",
    "\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Bar Revenue')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_forecasted = pred.predicted_mean #you'd add whatever other coefficients you want here. \n",
    "y_truth = chosen_bar_data[train_test_index:]\n",
    "\n",
    "# Compute the mean square error\n",
    "mse = ((y_forecasted - y_truth) ** 2).mean()\n",
    "print('The Mean Squared Error of our forecasts is {}'.format(mse))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Again don't be fooled, just because the mean squared error went down from 0.01 to 0.004 doesn't mean it could really predict with greater accuracy on future data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Dynamic forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pred_dynamic = results.get_prediction(start=pd.to_datetime(y_test.index[0]), dynamic=True, full_results=True)\n",
    "pred_dynamic_ci = pred_dynamic.conf_int()\n",
    "\n",
    "ax = chosen_bar_data[mid_data_index:].plot(label='observed', figsize=(20, 15))\n",
    "pred_dynamic.predicted_mean.plot(label='Dynamic Forecast', ax=ax)\n",
    "\n",
    "ax.fill_between(pred_dynamic_ci.index,\n",
    "                pred_dynamic_ci.iloc[:, 0],\n",
    "                pred_dynamic_ci.iloc[:, 1], color='k', alpha=.25)\n",
    "\n",
    "ax.fill_betweenx(ax.get_ylim(), pd.to_datetime(mid_data_index), bar_data.index[-1],\n",
    "                 alpha=.1, zorder=-1)\n",
    "\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Bar performance')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# So where do we go from here? \n",
    "Simply taking out the high P values won't necessarily help because by taking out one variable, the p values of the other parameters change as well. \n",
    "\n",
    "I've decided to use a machine learning technique called ElasticNet Regression to shed some light on what variables (if any) help us in our predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Elastic what?\n",
    "\n",
    "Elastic regression is a combination of a ridge regression and a lasso regression.\n",
    "\n",
    "# Ridge?\n",
    "\n",
    "In a nutshell standard ordinary least squares (OLS) regressions don't typically do well with high dimensional problems.\n",
    "This is because OLS gives the smallest mean squared error among linear estimators with NO BIAS.\n",
    "Ridge regression tries to lower the error even more by adding more of a bias. \n",
    "\n",
    "# Lasso?\n",
    "\n",
    "Lasso is an acronym for \"Least Absolute Selection and Shrinkage Operator\"\n",
    "The only difference between lasso and ridge is that Lasso use what's called the L2 penalty and Ridge uses the L1 penalty. \n",
    "I can talk about the difference in a future post, but to be brief, the lasso can eliminate parameters that aren't statistically significant. Whereas the ridge regression simply keeps lowerering their coefficients but never turning them to 0. \n",
    "\n",
    "\n",
    "# And now the ElasticNet\n",
    "\n",
    "It's a combination of ridge and the lasso. The reasoning is that when you're dealing with lots of variables, you'll often get a lot of them which are correlated with each other and form a sort of cluster if you will. Often times we will want these entire clusters of parameters to remain together should you choose any one of them. If you include one of the parameters you should include them all, if you delete one, you should delete them all. \n",
    "\n",
    "You can see it as the elimination properties of the lasso combined with the more lax \"reduce but don't eliminate\" method of the ridge regression. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Testing for stationarity...\n",
    "\n",
    "Like before, before doing any analysis, let's make sure our series is stationary. \n",
    "\n",
    "Remember that the series we had the most success predicting on the Sarimax model was the revenue relative to its 2 year average (revenueRelAvg)\n",
    "\n",
    "And so we will continue with that series and simply try to find the best hockey variables that could help us in our prediction. We will then try those variables on the Sarimax model once more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#chosen_bar_data=smaller_business_data[\"revenueRelAvg\"]\n",
    "\n",
    "differenced=False\n",
    "logged=False\n",
    "\n",
    "    \n",
    "#if logged:\n",
    "#    chosen_bar_data=np.log1p(chosen_bar_data)\n",
    "\n",
    "    \n",
    "if differenced:\n",
    "    differenced_y=chosen_bar_data-chosen_bar_data.shift()\n",
    "    differenced_y.dropna(inplace=True)\n",
    "\n",
    "else:\n",
    "    differenced_y=chosen_bar_data\n",
    "\n",
    "    \n",
    "if logged:\n",
    "    chosen_bar_data=np.log1p(chosen_bar_data)\n",
    "\n",
    "test_stationarity(differenced_y)\n",
    "\n",
    "\n",
    "#elasticNet parameters\n",
    "l1_ratio=0.5\n",
    "alphas = np.arange(0.0, 3, 0.05)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Great! It's stationary, we didn't need to difference it nor log it. \n",
    "\n",
    "Now we'll try training our ElasticNet just using the hockey data, no lags just to see if anything pops up.\n",
    "\n",
    "Here we split up the hockey data into its training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNetCV,ElasticNet #if you want lasso, just put l1_ratio=1 in ElasticNet Config\n",
    "#Constant that multiplies the penalty terms alpha of 0 is more or less equiavlent to doing OLS\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "train_pct=0.8\n",
    "test_pct=1-train_pct\n",
    "\n",
    "x=smaller_hockey_df.loc[differenced_y.index]\n",
    "\n",
    "data_dict=train_test(train_pct,x=x,y=differenced_y)\n",
    "x_train=data_dict[\"x_train\"]\n",
    "x_test=data_dict[\"x_test\"]\n",
    "y_train=data_dict[\"y_train\"]\n",
    "y_test=data_dict[\"y_test\"]\n",
    "mid_data_index=data_dict[\"mid_data_index\"]\n",
    "all_y=differenced_y\n",
    "all_x=x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The following function was taken from https://www.kaggle.com/apapiu/regularized-linear-models. It allows us to see how much regularization (which you configure in the alpha variable) is best for us. \n",
    "\n",
    "Regularization is basically a trade off between overfitting too much (where you infer too much from your training data) and over generalizing (the most extreme being just a straight line)\n",
    "\n",
    "We set this through cross-validation which splits up your dataset into different train and test batches to see how well your model would have done had the training and testing sets been different. \n",
    "\n",
    "In our case, because we're dealing with a timeseries, we'll be using a special type of cross-validation that maintains the order in time of the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def rmse_cv(model,x_train, y_train,cv=5):\n",
    "    #neg_mean_squared_error, neg_mean_absolute_error\n",
    "    rmse= np.sqrt(-cross_val_score(model, x_train, y_train, scoring=\"neg_mean_squared_error\", cv = 5))\n",
    "\t#with scoring being blank, by default this would've outputted the accuracy, ex: 95%\n",
    "\t#with scoring=\"neg_mean_squared_error\", we get accuracy -1, so shows by how much you were off and it's negative\n",
    "\t#then with the - in front, gives you the error, but positive. \n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "cv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "cv_elastic = [rmse_cv(ElasticNet(normalize =False,alpha = alpha,l1_ratio=l1_ratio,max_iter=1000),x_train, y_train,cv=cv).mean() for alpha in alphas]\n",
    "\n",
    "cv_elastic = pd.Series(cv_elastic, index = alphas)\n",
    "cv_elastic.plot(title = \"Validation\")\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"rmse\")\n",
    "\n",
    "chosen_alpha=cv_elastic.idxmin() #where the error is lowest\n",
    "\n",
    "print(\"alpha chosen at %f\" %chosen_alpha)\n",
    "\n",
    "\n",
    "model_elastic=ElasticNet(normalize =False,alpha = chosen_alpha,l1_ratio=l1_ratio,max_iter=1000).fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "#FEATURE SELECTION\n",
    "#getting an idea of what's important and what isn't.  \n",
    "\n",
    "features=x_train.columns\n",
    "coef = pd.Series(model_elastic.coef_, index = features)\n",
    "nber_of_variables=sum(coef != 0)\n",
    "\n",
    "print(\"Lasso picked \" + str(nber_of_variables) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")\n",
    "\n",
    "print(\"score of %f\"%model_elastic.score(x_train,y_train))\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "coef.abs().sort_values()[-nber_of_variables:].plot(kind = \"barh\")\n",
    "\n",
    "plt.title(\"Coefficients in the Lasso Model\")\n",
    "\n",
    "#let's say we keep \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Well, it looks like the elastic net picked up only variable that was statistically significant on its own. But even at that, it seems very weak, just look at that coefficient..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Plotting the differenced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "pred=pd.DataFrame(model_elastic.predict(x),index=x.index)\n",
    "\n",
    "answer=pd.concat([pred,differenced_y],axis=1)\n",
    "\n",
    "answer.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#This is to put it back into its original measurement.\n",
    "def undo_differencing(original_y,prediction):\n",
    "    original_y=pd.DataFrame(original_y)\n",
    "    prediction=pd.DataFrame(prediction)\n",
    "    answer = pd.Series(original_y.iloc[0].values, index=original_y.index)   \n",
    "\n",
    "    combined=pd.concat([answer,prediction.cumsum()],axis=1).fillna(0)\n",
    "\n",
    "    combined=combined.sum(axis=1)\n",
    "    return combined\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In case I had differend the series or logged it, I would undo them here, so as to see what our final prediction would have been. Also here we would've had a step forward prediciton, whereby we'd be using the a real observation at each step to help predict the next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#STEP FORWARD PREDICTION\n",
    "\n",
    "prediction=model_elastic.predict(x)\n",
    "prediction=pd.Series(prediction,index=x.index)\n",
    "\n",
    "\n",
    "if logged:\n",
    "    prediction=np.exp(prediction)-1\n",
    "\n",
    "if differenced:\n",
    "    final_pred=(prediction+chosen_bar_data.shift()).dropna()\n",
    "\n",
    "else:    \n",
    "    #WITHOUT DIFFERENCING\n",
    "    final_pred=pd.Series(prediction,index=x.index)\n",
    "    #final_pred=np.exp(final_pred)\n",
    "\n",
    "    \n",
    "answer=pd.concat([final_pred,chosen_bar_data],axis=1,ignore_index=True)\n",
    "\n",
    "#answer\n",
    "\n",
    "forecast_mse = ((final_pred - chosen_bar_data) ** 2).mean()\n",
    "baseline_mse = ((chosen_bar_data-chosen_bar_data.shift()) ** 2).mean()\n",
    "no_guess_mse=((chosen_bar_data) ** 2).mean()\n",
    "\n",
    "print(\"My forecast MSE : {}, \\nsimply using previous value MSE : {}, \\nstraight line MSE :{}\".format(forecast_mse,baseline_mse,no_guess_mse))\n",
    "\n",
    "\n",
    "answer.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As you can see, based on the montreal goals, it's only willing to predict so much one way or another. Still it is interesting to note that it's performing better than simply a straight line through 0 as well as simply predicting the previous value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#DYNAMIC PREDICTION\n",
    "if logged:\n",
    "    prediction=np.exp(prediction)-1\n",
    "\n",
    "    \n",
    "if differenced:\n",
    "    final_pred=undo_differencing(chosen_bar_data,pd.Series(prediction,index=x.index))\n",
    "\n",
    "else:    \n",
    "    #WITHOUT DIFFERENCING\n",
    "    final_pred=pd.Series(prediction,index=x.index)\n",
    "    #final_pred=np.exp(final_pred)\n",
    "\n",
    "answer=pd.concat([final_pred,chosen_bar_data],axis=1,ignore_index=True)\n",
    "\n",
    "#answer\n",
    "\n",
    "forecast_mse = ((final_pred - chosen_bar_data) ** 2).mean()\n",
    "\n",
    "baseline_mse = ((chosen_bar_data-chosen_bar_data.shift()) ** 2).mean()\n",
    "\n",
    "no_guess_mse=((chosen_bar_data) ** 2).mean()\n",
    "\n",
    "print(\"My forecast MSE : {}, \\nsimply using previous value MSE : {}, \\nstraight line MSE :{}\".format(forecast_mse,baseline_mse,no_guess_mse))\n",
    "\n",
    "answer.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "If we were dealing with differenced data, above we'd have the dynamic prediction whereby we predict the next value based on our previously predicted values for the current period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# ElasticNet With Lags\n",
    "\n",
    "Now we'll see if adding past quarerly revenue data helps in making our prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#list_of_lags=list(range(1,5)\n",
    "def make_lags(list_of_lags,y_serie):\n",
    "    data=pd.DataFrame()\n",
    "    for each in list_of_lags:\n",
    "        lag=y_serie.shift(each)\n",
    "        lag=lag.rename(\"lag %i\"%each)\n",
    "        data=pd.concat([data,lag],axis=1)\n",
    "    data.dropna(inplace=True)\n",
    "    return data\n",
    "\n",
    "y_train_lags=make_lags(list(range(1,5)),y_train)\n",
    "y_test_lags=make_lags(list(range(1,5)),y_test)\n",
    "all_y_lags=make_lags(list(range(1,5)),all_y)\n",
    "\n",
    "x_train_with_lags=pd.concat([x_train,y_train_lags],axis=1)\n",
    "x_train_with_lags.dropna(inplace=True)\n",
    "x_test_with_lags=pd.concat([x_test,y_test_lags],axis=1)\n",
    "x_test_with_lags.dropna(inplace=True)\n",
    "all_x_with_lags=pd.concat([x_train,y_train_lags],axis=1)\n",
    "all_x_with_lags.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#because of the lags, you can't compare with all of y_train, the lags will lower the number of data points for your input\n",
    "dates_in_common_train=np.intersect1d(x_train_with_lags.index.values, y_train.index.values)\n",
    "y_train_lagged=y_train.loc[dates_in_common_train]\n",
    "\n",
    "dates_in_common_test=np.intersect1d(x_test_with_lags.index.values, y_test.index.values)\n",
    "y_test_lagged=y_test.loc[dates_in_common_test]\n",
    "\n",
    "dates_in_common_all=np.intersect1d(all_x_with_lags.index.values, all_y.index.values)\n",
    "all_y_lagged=all_y.loc[dates_in_common_all]\n",
    "\n",
    "chosen_bar_data_lagged=chosen_bar_data.loc[dates_in_common_all]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "cv_elastic = [rmse_cv(ElasticNet(normalize =False,alpha = alpha,l1_ratio=l1_ratio,max_iter=1000),x_train_with_lags, y_train_lagged,cv=cv).mean() for alpha in alphas]\n",
    "\n",
    "cv_elastic = pd.Series(cv_elastic, index = alphas)\n",
    "cv_elastic.plot(title = \"Validation\")\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"rmse\")\n",
    "\n",
    "chosen_alpha=cv_elastic.idxmin() #where the error is lowest\n",
    "\n",
    "print(\"alpha chosen at %f\" %chosen_alpha)\n",
    "\n",
    "\n",
    "model_elastic=ElasticNet(normalize =False,alpha = chosen_alpha,l1_ratio=l1_ratio,max_iter=1000).fit(x_train_with_lags, y_train_lagged)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#FEATURE SELECTION\n",
    "#getting an idea of what's important and what isn't.  \n",
    "features=x_train_with_lags.columns\n",
    "coef = pd.Series(model_elastic.coef_, index = features)\n",
    "nber_of_variables=sum(coef != 0)\n",
    "\n",
    "print(\"Lasso picked \" + str(nber_of_variables) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")\n",
    "\n",
    "print(\"score of %f\"%model_elastic.score(x_train_with_lags,y_train_lagged))\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "coef.abs().sort_values()[-nber_of_variables:].plot(kind = \"barh\")\n",
    "\n",
    "\n",
    "if nber_of_variables==0:\n",
    "    winning_coef=[\"\"]\n",
    "else:\n",
    "    winning_coef=coef.abs().sort_values()[-nber_of_variables:].index.values\n",
    "    winning_coef=winning_coef[::-1]\n",
    "\n",
    "\n",
    "plt.title(\"Coefficients in the Lasso Model\")\n",
    "\n",
    "#let's say we keep \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Interesting! It seems that even with the lags, the elastnet only picked the MtlGoals variable again. The code below will basically just be a repeat of what's found above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#STEP FORWARD PREDICTION\n",
    "\n",
    "prediction=model_elastic.predict(all_x_with_lags)\n",
    "prediction=pd.Series(prediction,index=all_x_with_lags.index)\n",
    "\n",
    "if logged:\n",
    "    prediction=np.exp(prediction)-1\n",
    "\n",
    "if differenced:\n",
    "    final_pred=(prediction+chosen_bar_data_lagged.shift()).dropna()\n",
    "\n",
    "else:    \n",
    "    #WITHOUT DIFFERENCING\n",
    "    final_pred=pd.Series(prediction,index=all_x_with_lags.index)\n",
    "    #final_pred=np.exp(final_pred)\n",
    "\n",
    "    \n",
    "answer=pd.concat([final_pred,chosen_bar_data_lagged],axis=1,ignore_index=True)\n",
    "\n",
    "\n",
    "#answer\n",
    "\n",
    "forecast_mse = ((final_pred - chosen_bar_data_lagged) ** 2).mean()\n",
    "baseline_mse = ((all_y_lagged-chosen_bar_data_lagged.shift()) ** 2).mean()\n",
    "no_guess_mse=((chosen_bar_data_lagged) ** 2).mean()\n",
    "\n",
    "print(\"My forecast MSE : {}, \\nsimply using previous value MSE : {}, \\nstraight line MSE :{}\".format(forecast_mse,baseline_mse,no_guess_mse))\n",
    "\n",
    "\n",
    "answer.plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#DYNAMIC PREDICTION\n",
    "\n",
    "prediction=model_elastic.predict(all_x_with_lags)\n",
    "prediction=pd.Series(prediction,index=all_x_with_lags.index)\n",
    "\n",
    "if logged:\n",
    "    prediction=np.exp(prediction)-1\n",
    "\n",
    "    \n",
    "if differenced:\n",
    "    final_pred=undo_differencing(chosen_bar_data_lagged,pd.Series(prediction,index=all_x_with_lags.index))\n",
    "\n",
    "else:    \n",
    "    #WITHOUT DIFFERENCING\n",
    "    final_pred=pd.Series(prediction,index=all_x_with_lags.index)\n",
    "    #final_pred=np.exp(final_pred)\n",
    "\n",
    "    \n",
    "\n",
    "answer=pd.concat([final_pred,chosen_bar_data_lagged],axis=1).dropna()\n",
    "\n",
    "#answer\n",
    "forecast_mse = ((final_pred - chosen_bar_data_lagged) ** 2).mean()\n",
    "baseline_mse = ((chosen_bar_data_lagged-chosen_bar_data_lagged.shift()) ** 2).mean()\n",
    "no_guess_mse=((chosen_bar_data_lagged) ** 2).mean()\n",
    "\n",
    "print(\"My forecast MSE : {}, \\nsimply using previous value MSE : {}, \\nstraight line MSE :{}\".format(forecast_mse,baseline_mse,no_guess_mse))\n",
    "\n",
    "answer.plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Back to Sarimax\n",
    "\n",
    "Ok, well it's weird that the lag didn't pop up as an important variable, but it seems like MtlGoals is one of them. Let's put that in our model and see if it improves things. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define the p, d and q paradmeters to take any value between 0 and 2\n",
    "p =  q = range(0, 4)\n",
    "d =range(0, 2)\n",
    "# Generate all different combinations of p, q and q triplets\n",
    "pdq = list(itertools.product(p, d, q))\n",
    "\n",
    "\n",
    "# Generate all different combinations of seasonal p, q and q triplets\n",
    "seasonal_pdq = [(x[0], x[1], x[2], 4) for x in list(itertools.product(p, d, q))]\n",
    "\n",
    "print('Examples of parameter combinations for Seasonal ARIMA...')\n",
    "print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Like before you can run the gridsearch if you want, but the answer has been saved in the next cell. It's \n",
    "\n",
    "\n",
    "BEST FIT\n",
    "ARIMA(2, 0, 1)x(0, 0, 0, 4)4 - AIC:-73.46538552275928\n",
    "parameters in play\n",
    "['MtlGoals']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "do_sarimax=False\n",
    "if do_sarimax==True:    \n",
    "    #cols_I_want=winning_coef\n",
    "    cols_I_want=[\"MtlGoals\"]   # wednesdayRelAvgDifferenced sundayRelAvgDifferenced ,\"Home\"\n",
    "    cols_in_play=[]\n",
    "    best_score, best_cfg = float(\"inf\"), None\n",
    "    for each_col in cols_I_want:\n",
    "        cols_in_play.append(each_col)\n",
    "        print(each_col)\n",
    "        x_columns=x_train[cols_in_play]\n",
    "        other_y=smaller_business_data[\"revenueRollingNormalization\"][x_columns.index]\n",
    "\n",
    "        warnings.filterwarnings(\"ignore\") # specify to ignore warning messages\n",
    "\n",
    "        for param in pdq:\n",
    "            for param_seasonal in seasonal_pdq:\n",
    "\n",
    "                try:\n",
    "                    mod = sm.tsa.statespace.SARIMAX(y_train,\n",
    "                                            #other_y,\n",
    "                                            order=param,#order=(1, 0, 1),\n",
    "                                            exog =x_columns, #putting in other regressors\n",
    "                                            #exog=None,        \n",
    "                                            seasonal_order=param_seasonal,#seasonal_order=(1, 0, 1, 12),\n",
    "                                            enforce_stationarity=False,\n",
    "                                            enforce_invertibility=False)\n",
    "\n",
    "                    results = mod.fit()\n",
    "\n",
    "\n",
    "                    if results.aic< best_score:\n",
    "                        best_score=results.aic\n",
    "                        best_cfg=[param, param_seasonal]\n",
    "\n",
    "                        parameters_in_play=cols_in_play.copy()\n",
    "                        print('ARIMA{}x{}4 - AIC:{}'.format(param, param_seasonal, results.aic))\n",
    "                        #print(\"best parameters so far\")\n",
    "                        #print(parameters_in_play)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "    print(\"BEST FIT\")            \n",
    "    print('ARIMA{}x{}4 - AIC:{}'.format(best_cfg[0], best_cfg[1], best_score))\n",
    "    print(\"parameters in play\")\n",
    "    print(parameters_in_play)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cols_in_play=[]\n",
    "cols_I_want=[\"MtlGoals\"]\n",
    "#cols_I_want=parameters_in_play\n",
    "x_columns=all_x[cols_I_want]\n",
    "\n",
    "\n",
    "best_cfg=[(2, 0, 1),(0, 0, 0, 4)]\n",
    "\n",
    "#with no hockey,\n",
    "#best_cfg=[(2, 0, 1), (0, 0, 0, 4)]   \n",
    "\n",
    "\n",
    "mod = sm.tsa.statespace.SARIMAX(all_y,\n",
    "                                order=best_cfg[0],#order=(1, 0, 1),\n",
    "                                exog =x_columns, #putting in other regressors\n",
    "                                #exog=None,\n",
    "                                seasonal_order=best_cfg[1],#seasonal_order=(1, 0, 1, 12),\n",
    "                                enforce_stationarity=False,\n",
    "                                enforce_invertibility=False)\n",
    "\n",
    "results = mod.fit()\n",
    "print(results.summary())\n",
    "\n",
    "results.plot_diagnostics(figsize=(15, 12))\n",
    "plt.show()\n",
    "\n",
    "print(\"PREDICTION NOT DYNAMIC\")\n",
    "pred = results.get_prediction(start=pd.to_datetime(y_test.index[0]), dynamic=False)\n",
    "#The dynamic=False argument ensures that we produce one-step ahead forecasts, meaning that forecasts at each point are generated using the full history up to that point.\n",
    "pred_ci = pred.conf_int()\n",
    "\n",
    "ax = all_y[mid_data_index:].plot(label='observed')\n",
    "pred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7)\n",
    "\n",
    "ax.fill_between(pred_ci.index,\n",
    "                pred_ci.iloc[:, 0],\n",
    "                pred_ci.iloc[:, 1], color='k', alpha=.2)\n",
    "\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Bar Revenue')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"TEST ERRORS ANALYSIS\")\n",
    "\n",
    "\n",
    "\n",
    "y_forecasted = pred.predicted_mean #you'd add whatever other coefficients you want here. \n",
    "y_truth=y_test_lagged\n",
    "\n",
    "\n",
    "forecast_mse = ((y_forecasted - y_truth) ** 2).mean()\n",
    "\n",
    "baseline_mse = ((y_truth-y_truth.shift()) ** 2).mean()\n",
    "\n",
    "no_guess_mse=((y_truth) ** 2).mean()\n",
    "\n",
    "print(\"My forecast MSE : {}, \\nsimply using previous value MSE : {}, \\nstraight line MSE :{}\".format(forecast_mse,baseline_mse,no_guess_mse))\n",
    "\n",
    "\n",
    "print(\"PREDICTION DYNAMIC\")\n",
    "pred_dynamic = results.get_prediction(start=pd.to_datetime(y_test.index[0]), dynamic=True, full_results=True)\n",
    "\n",
    "pred_dynamic_ci = pred_dynamic.conf_int()\n",
    "\n",
    "ax = all_y[mid_data_index:].plot(label='observed', figsize=(20, 15))\n",
    "pred_dynamic.predicted_mean.plot(label='Dynamic Forecast', ax=ax)\n",
    "\n",
    "ax.fill_between(pred_dynamic_ci.index,\n",
    "                pred_dynamic_ci.iloc[:, 0],\n",
    "                pred_dynamic_ci.iloc[:, 1], color='k', alpha=.25)\n",
    "\n",
    "ax.fill_betweenx(ax.get_ylim(), pd.to_datetime(mid_data_index), bar_data.index[-1],\n",
    "                 alpha=.1, zorder=-1)\n",
    "\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Bar performance')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"TEST ERRORS ANALYSIS\")\n",
    "\n",
    "y_forecasted = pred_dynamic.predicted_mean\n",
    "#y_truth = other_y_test #from the test going forward only\n",
    "\n",
    "forecast_mse = ((y_forecasted - y_truth) ** 2).mean()\n",
    "baseline_mse = ((y_truth-y_truth.shift()) ** 2).mean()\n",
    "no_guess_mse=((y_truth) ** 2).mean()\n",
    "\n",
    "print(\"My forecast MSE : {}, \\nsimply using previous value MSE : {}, \\nstraight line MSE :{}\".format(forecast_mse,baseline_mse,no_guess_mse))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Above you'll se both the regulard and dynamic regressions. It looks like the best model including the MtlGoals variable outperforms ever so slightly our previous model that didn't have any hockey data at all. \n",
    "\n",
    "And also look at those P values, they are all below 0.05.\n",
    "\n",
    "\n",
    "\n",
    "And finally the Sarimax model configurations [(2, 0, 1),(0, 0, 0, 4)]\n",
    "\n",
    "It looks like the optimal model involved the use of 2 lags for autocorrelation and 1 lag for the moving average.\n",
    "\n",
    "Interestingly enough nothing was done with seasonality.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Here are the final results\n",
    "\n",
    "Mean squared error (Not Dynamic)\n",
    "\n",
    "With MtlGoals: 0.00707 --- Without MtlGoals 0.0100\n",
    "\n",
    "Mean squared error (Dynamic) \n",
    "\n",
    "With MtlGoals: 0.00854 --- Without MtlGoals 0.0175\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "And there you have it, when predicting the quarterly revenue of Sportscene relative to its 2 year average, it seems the number of goals scored by the Montreal Canadiens team does help in predicting the next quarterly revenue at least a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
