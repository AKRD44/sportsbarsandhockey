{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "Like many of you, whenever the Montreal Canadiens hockey team have a match, it can happen that my compadres and I might find ourselves in a sports bar. \n",
    " \n",
    "Guzzling down pints and stuffing my face with burgers among my fellow hockey fans, I sometimes wonder about the relationship between our beloved sports team’s performance on ice and how it affects our performance with the pints and burgers (i.e how much revenue sport bars make). \n",
    " \n",
    "Have you thought about that too? Of course you have.  Because you are of the numerically inclined! Which is why your native curiousity let you here to my humble page. \n",
    "\n",
    "So let us see if how the Montreal Canadiens are performing determine how well Montreal sports bars are doing.\n",
    "\n",
    "About the data:\n",
    " \n",
    "La Cage aux Sports, owned by The Sportscene Group is one of the biggest sport bars in Quebec and only operates in Quebec. I decided to collect their quarterly revenues from their financial statements which I found on http://www.sedar.com\n",
    " \n",
    "As for the Hockey data, I got it from http://ourhistory.canadiens.com/stats/search#/dashboard/players/\n",
    " \n",
    "I will first take a look at the sport bar data alone and see what I can make of it. Then I will bring in the hockey data and see if it adds anything.\n",
    "\n",
    "So let’s take a look! \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "At first I'm just going to load up the data and wrangle it a little bit in order to to get started. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'sportscenedata/sportscene.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-cf9f14181ac4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmyscripts\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sportscenedata/sportscene.csv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"date\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"quarter\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"half\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"year\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alexkeenan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    644\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    645\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 646\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alexkeenan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alexkeenan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    728\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 730\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    731\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    732\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alexkeenan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    921\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    922\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 923\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    924\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    925\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alexkeenan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1388\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1390\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1392\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas\\parser.c:4184)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas\\parser.c:8449)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'sportscenedata/sportscene.csv' does not exist"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import warnings\n",
    "import itertools\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#plt.style.use(\"seaborn\")\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15, 6)\n",
    "from myscripts import *\n",
    "\n",
    "data=pd.read_csv(\"sportscene.csv\",index_col=\"date\",parse_dates=True)\n",
    "\n",
    "data.columns=[\"quarter\",\"half\",\"year\"]\n",
    "data.sort_index(inplace=True)\n",
    "\n",
    "data=data.groupby(data.index).first()\n",
    "\n",
    "data=data.iloc[:,[0,2]]\n",
    "data.iloc[:,0]=data.iloc[:,0].str.replace(\",\",\"\")\n",
    "\n",
    "data=data.apply(pd.to_numeric)\n",
    "data=data.fillna(0)\n",
    "data[\"revenue\"]=data.iloc[:,0]+data.iloc[:,1]\n",
    "\n",
    "yearly_data=data.revenue[data.index.month==8]\n",
    "quarters_data=data.quarter[data.index.month!=8]\n",
    "\n",
    "if quarters_data.shape[0]%3!=0:\n",
    "    quarters_data=quarters_data.iloc[:-(quarters_data.shape[0]%3)] #not done with current year,\n",
    "\n",
    "\n",
    "grouped_up=np.split(quarters_data, quarters_data.shape[0]/3)\n",
    "\n",
    "totals=[]\n",
    "for tmp in grouped_up:\n",
    "    totals.append(tmp.sum())\n",
    "    \n",
    "#Every 4rth bar revenue data point shows the revenue for the entire year, so to find how that quarter did, need to difference with the rest\n",
    "\n",
    "data.revenue[data.index.month==8]=data.revenue[data.index.month==8].values-totals\n",
    "\n",
    "#resampling to MONTHLY so as to get the end of month\n",
    "data=data.revenue.resample(\"M\").sum()\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "all_dates_inplay=data.index.date\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "There! Now let's see what sportscene revenue looks like on a quarterly basis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Pretty choppy eh? We'll get a clearer look as to what's behind this real soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Creating stats for bar data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "window=8\n",
    "bar_data_stats=create_stats(pd.DataFrame(data),window,rolling_mean_shifted=True)\n",
    "bar_data=pd.concat([data,bar_data_stats],axis=1)\n",
    "bar_data.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "I above function creates statistics based on the inputed data. \n",
    "\n",
    "The window parameter is used for the rolling average calculated within. A window of 8 means the calculated mean will be of the 8 past data points. 8 quarters means two years. \n",
    "\n",
    "To give you a quick idea, here is what they are called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(bar_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "nber_of_graphs=bar_data.columns.shape[0]\n",
    "graphs_per_row=2\n",
    "nber_of_rows= np.ceil(nber_of_graphs/graphs_per_row)\n",
    "\n",
    "nber_of_rows_number=nber_of_rows*100\n",
    "graphs_per_row_number=graphs_per_row*10\n",
    "\n",
    "i=1\n",
    "base_number=nber_of_rows_number+graphs_per_row_number\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (15, 3*nber_of_rows)\n",
    "#['revenueDifferencedRollingStandardization','revenueRollingNormalization','revenueDifferencedRollingNormalization']:\n",
    "    \n",
    "for each_column in bar_data.columns:    \n",
    "\n",
    "    plt.subplot(base_number+i) \n",
    "    plt.plot(bar_data[each_column])\n",
    "    plt.title(each_column)\n",
    "    i+=1\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Above you can see what the extra columns look like. They're just variations of the original revenue input. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (15, 9)\n",
    "\n",
    "chosen_bar_data=bar_data.revenueRelAvg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's take a closer look at the revenueRelAvg one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "chosen_bar_data.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "chosen_bar_data.groupby(chosen_bar_data.index.month).mean().plot(kind=\"bar\",title=\"Quarterly revenue relative to %.2f year average\"%(window/4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "So this is a little better. We can clearly see that the third quarter is typically the weakest.\n",
    "\n",
    "What I also see is that there seems to be a pattern. I could easily divide this into half years and the \"first\" part would always be lower than the second one. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now this data across time is what's called a time series. The key thing to remember is that you can't just run a regression on it straight off the bat. Regressions along with many other statistical tests assume the data points are independent of each other and that the probability distribution of the data and its variance remain the same throughout (stationarity). \n",
    "\n",
    "In this case they are not independent as they all depend on time. Therefore special models like ARIMA and SARIMAX are needed.\n",
    "\n",
    "As for stationarity, if we were to look at the quarterly revenue, it seems to increase gradually while never reverting to a mean. In theory it could go on to infinity, which makes this definitely non-stationary. Now I could difference the revenue, but I'll keep going with with the quarterly revenue relative to its 2 year mean that I've got. \n",
    "\n",
    "By making it relative to its average, I've restrained where the value could go. You'll notice it'll always revert back to 0. \n",
    "Take a look below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#log them to help with the stationarity\n",
    "#stationary_data=np.log1p(bar_data[[0]]-bar_data[[0]].shift())\n",
    "\n",
    "#stationary_data=chosen_bar_data-chosen_bar_data.shift()\n",
    "stationary_data=chosen_bar_data\n",
    "\n",
    "stationary_data.dropna(inplace=True)\n",
    "stationary_data.plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "Let's test it for stationarity to make sure. The best method for detecting stationarity is with what's called the  the Dickey-Fuller test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_stationarity(stationary_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Using the Dickey-Fuller test we can see that our test statistic is even lower than the 1% critical value, meaning there's less than a 1% chance that we would've gotten these results and that the series was not stationary. \n",
    "\n",
    "\n",
    "Moving on, let us examine to see if our data has any autocorrelation, which is to say, do previous results help predict future results? There are quick ways to tell if there might be some."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lag_plot(pd.DataFrame(stationary_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This chart plots every data point along with its next data point as the coordinate for each dot. Had there been some strong autocorrelation amongst neighbouring values, some clear pattern might've emerged. In this case, it looks random. Now there still might be some autocorrelation, but it wouldn't be that strong. Either that or the autocorrelation might not be with its immediate neighbouring values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "time_series_info_on_y(stationary_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The first graph here is the Autocorrelation Function (ACF). It basically incorporates all the lags un until the selected value. So ACF(4) looks at how all the lags up until lag 4 correlate with our prediction.\n",
    "\n",
    "The second graph here shows the Partial Autocorrelation Function (PACF). This only focuses on one particular lag at a time. Notice here that it looks like there is some autocorrelation for the second lag.\n",
    "\n",
    "Anything outside of the shaded area implies some statistically significant relationship. Anything within the shaded area is considered noise. So the lag of 1 and the lag of 3 in the PACF for example don't seem significant.\n",
    "\n",
    "So here we see that lag 2 and 4 seems significant. This will be important later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Game plan\n",
    "\n",
    "We will first try to predict the next quarterly revenue purely based on its past revenue. Then I will try to incorporate hockey data and see if that improves things.\n",
    "\n",
    "We will be using what's called a SARIMAX model and then we will try a machine learning technique called the ElasticNet regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    " \n",
    "# Going the SARIMAX route below. No exogenous variables, just pure lags\n",
    "\n",
    "Before tackling SARIMAX, what is an ARIMA model? ARIMA stands for AutoRegressive Integrated Moving Average.\n",
    "It allows us to take in raw time series apply transformations to make it stationary, analyze and then come up with a prediction. \n",
    "\n",
    "The variables it takes in are ARIMA(p,d,q). \n",
    "\n",
    "p stands for how many lagged values we want to take into account when predicting the next value.\n",
    "This speaks directly to the AutoRegressive component of the model. \n",
    "For example a lag of 1 (AR(1)) means you are using today's temperature to predict tomorrow's temperature. \n",
    "AR(2) means tomorrow's temperature is correlated with yesterday and today's temperature.\n",
    "\n",
    "d is for differencing. Which explains the integrated part of the arima model. The number d just refers to how many times you differenced your data. So a difference of 1 means Y2-Y1, Y3-Y2. So basically instead of having the temperature of everyday, you have the difference in temperature compared to that of the previous day. If d=2, then you'd be dealing with the difference of the difference in temperatures.\n",
    "\n",
    "q is for moving average (MA). The key thing to keep in mind is that we are looking at the ERRORS for \"q\" lags and trying to see if they correlate with our prediction. What are those errors you might ask?\n",
    "\n",
    "When we do an AR(2) process, we try to predict using 2 lags. Naturally when we fit these with lots of data, we never get an exact fit. We get errors for each lags. This is the noise, the residual. These errors in our predictions are what we'll be using for MA(2).\n",
    "If the noise is completely random then it is uninformative. But in some cases there will still be some structure or information left in them, which is why we might have to take them into account in order to better predict.\n",
    "\n",
    "And finally we have SARIMAX. \n",
    "\n",
    "It is simply an ARIMA model but with seasonality in mind. A business typically has high and low seasons. And so if I want to predict my sales for the summer, the best predictor might be last year's summer, but not so much all the time/lags in between. The model has extra parameters P,D,Q which do the same things as p,d,q but compared across their matching seasons.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Define the p, d and q parameters\n",
    "\n",
    "There is a whole lot of literature on how to choose your parameters. \n",
    "View this link for a taste.\n",
    "https://people.duke.edu/~rnau/411sdif.htm\n",
    "It can take a lot of time to do it manually. So what I've done is simply used a grid search that looks at all possible combinations and tries them all. The best model survives.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Below we split our data into train and test sets. We won't be using those pre-made functions from popular libraries like sklearn preprocessing because from what I've read it randomizes the datapoints before assigning what is for training and what is for testing. This would be great were our data points independent of each other. In our case with timeseries, they are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#make sure to also check window\n",
    "train_pct=0.8\n",
    "split_data=train_test(train_pct,y=bar_data)\n",
    "\n",
    "y_train=split_data[\"y_train\"]\n",
    "y_test=split_data[\"y_test\"]\n",
    "mid_data_index=split_data[\"mid_data_index\"]\n",
    "y_all=bar_data\n",
    "train_test_index=split_data[\"train_test_index\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The following code was based on this terrific guide right here https://www.digitalocean.com/community/tutorials/a-guide-to-time-series-forecasting-with-arima-in-python-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "p=q = range(0,4)\n",
    "\n",
    "d=range(0,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Generating all different combinations of p, q and q triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pdq = list(itertools.product(p, d, q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Generate all different combinations of seasonal p, q and q triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "seasonal_pdq = [(x[0], x[1], x[2], 2) for x in list(itertools.product(p, d, q))]\n",
    "\n",
    "print('Examples of parameter combinations for Seasonal ARIMA...')\n",
    "print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You can set do_sarimax=True if you want to run the grid search, but it could take a while. By default we set it to the answer the gridsearch would have provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "do_sarimax=False #in case I don't want to run this thing again.\n",
    "\n",
    "if do_sarimax==True:\n",
    "    warnings.filterwarnings(\"ignore\") # specify to ignore warning messages\n",
    "    best_score, best_cfg = float(\"inf\"), None\n",
    "    for each_measure_unit in ['revenue','revenueRollingMean','revenueRelAvg','revenueRollingStandardization','revenueRollingNormalization']:\n",
    "\n",
    "        for param in pdq:\n",
    "            for param_seasonal in seasonal_pdq:\n",
    "\n",
    "                try:\n",
    "                    mod = sm.tsa.statespace.SARIMAX(y_train[each_measure_unit],\n",
    "                                                    order=param,\n",
    "                                                    exog=None,\n",
    "                                                    seasonal_order=param_seasonal,\n",
    "                                                    enforce_stationarity=False,\n",
    "                                                    enforce_invertibility=False)\n",
    "\n",
    "                    results = mod.fit()\n",
    "\n",
    "\n",
    "                    if results.aic< best_score:\n",
    "                        best_score=results.aic\n",
    "                        best_cfg=[param, param_seasonal]\n",
    "                        best_measure_unit=each_measure_unit\n",
    "                        print('ARIMA{}x{} - AIC:{} with data {}'.format(param, param_seasonal, results.aic,each_measure_unit))\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "    print(\"BEST FIT\")            \n",
    "    print('ARIMA{}x{} - AIC:{} with data {}'.format(best_cfg[0], best_cfg[1], best_score,best_measure_unit))\n",
    "else:\n",
    "    print(\"skipping Sarimax\")\n",
    "    #ARIMA(2, 0, 1)x(0, 0, 0, 4)4 - AIC:-90.93117578195475 with data revenueRelAvg\n",
    "    best_measure_unit=\"revenueRelAvg\"\n",
    "    best_cfg=[(0, 0, 0), (1, 0, 1, 2)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We're judging the model with the Akaike Information Criterion (AIC score)\n",
    "\n",
    "I will not discuss this at length but the key thing to remember about AIC is that it balances complexity and accuracy. Adding more parameters or lags will can always make the model more precise. But does that extra parameter REALLY add enough to warrant adding that parameter in the first place? Is adding more complexity to the model worth that extra bit of accuracy? The AIC score will tell us. \n",
    "The better the model the lower the AIC score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finally it seem that model is best at predicting the relative average rolling mean (which is how the quarterly revenue compares to its two year average) which is very informative still.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# NOW TO ANALYZE THE BEST ONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "mod = sm.tsa.statespace.SARIMAX(y_all[best_measure_unit],\n",
    "                                order=best_cfg[0],#order=(1, 0, 1),\n",
    "                                seasonal_order=best_cfg[1],#seasonal_order=(1, 0, 1, 12),\n",
    "                                enforce_stationarity=False,\n",
    "                                enforce_invertibility=False)\n",
    "\n",
    "results = mod.fit()\n",
    "print(results.aic)\n",
    "print(results.summary().tables[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "So the best model to survive in this case used 1 seasonal lag and 1 seasonal moving average. I've selected seasonality of 2, meaning I am splitting my year into 2 and seeing if there are any similarities between the half-years.\n",
    "\n",
    "The p values are also at either at 0 or close to it which is a very strong sign for statistical significance. The p value tells you the odds of you having the results you have and the coefficient for you variable being 0 (your variable having no effect). So if the P value is next to 0, it's telling you there is virtually no chance that your variable has no effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Statsmodel the library used here has great charts that are automatically produced for you whenever you run your Sarimax models.\n",
    "results.plot_diagnostics(figsize=(15, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The top left, top right and bottom left charts basically plot the errors. We want to make sure that the errors are look somewhat normal (i.e follows a gaussian distribution). From the look of things, it's not a perfect Gaussian but it's close enough. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# FORECASTING\n",
    "Let's see what our predictions would look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "pred = results.get_prediction(start=pd.to_datetime(y_test.index[0]), dynamic=False)\n",
    "#The dynamic=False argument ensures that we produce one-step ahead forecasts, meaning that forecasts at each point are generated using the full history up to that point.\n",
    "pred_ci = pred.conf_int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "ax = y_all[best_measure_unit][mid_data_index:].plot(label='observed')\n",
    "pred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7)\n",
    "\n",
    "ax.fill_between(pred_ci.index,\n",
    "                pred_ci.iloc[:, 0],\n",
    "                pred_ci.iloc[:, 1], color='k', alpha=.2)\n",
    "\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Bar Revenue')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "\n",
    "# Computing the mean square error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_forecasted = pred.predicted_mean \n",
    "y_truth = y_test[best_measure_unit]\n",
    "\n",
    "forecast_mse = ((y_forecasted - y_truth) ** 2).mean()\n",
    "baseline_mse = ((y_truth-y_truth.shift()) ** 2).mean()\n",
    "no_guess_mse=((y_truth) ** 2).mean()\n",
    "\n",
    "print(\"My forecast MSE : {}, \\nsimply using previous value MSE : {}, \\nstraight line MSE :{}\".format(forecast_mse,baseline_mse,no_guess_mse))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This doesn't look too bad, but the main thing to keep in mind here is that our model is constantly being updated with the real observations that took place. \n",
    "\n",
    "Often times the TRUE test of a good model is how much it can predict when it is fed its own predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pred_dynamic = results.get_prediction(start=pd.to_datetime(y_test.index[0]), dynamic=True, full_results=True)\n",
    "pred_dynamic_ci = pred_dynamic.conf_int()\n",
    "\n",
    "ax = chosen_bar_data[mid_data_index:].plot(label='observed', figsize=(20, 15))\n",
    "pred_dynamic.predicted_mean.plot(label='Dynamic Forecast', ax=ax)\n",
    "\n",
    "ax.fill_between(pred_dynamic_ci.index,\n",
    "                pred_dynamic_ci.iloc[:, 0],\n",
    "                pred_dynamic_ci.iloc[:, 1], color='k', alpha=.25)\n",
    "\n",
    "ax.fill_betweenx(ax.get_ylim(), pd.to_datetime(mid_data_index), chosen_bar_data.index[-1],\n",
    "                 alpha=.1, zorder=-1)\n",
    "\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Bar performance')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_forecasted = pred_dynamic.predicted_mean \n",
    "y_truth = y_test[best_measure_unit]\n",
    "\n",
    "forecast_mse = ((y_forecasted - y_truth) ** 2).mean()\n",
    "baseline_mse = ((y_truth-y_truth.shift()) ** 2).mean()\n",
    "no_guess_mse=((y_truth) ** 2).mean()\n",
    "\n",
    "print(\"My forecast MSE : {}, \\nsimply using previous value MSE : {}, \\nstraight line MSE :{}\".format(forecast_mse,baseline_mse,no_guess_mse))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Not very good eh? Basically we can get some idea of where we'll be for the NEXT period. But anything farther than that and the model has no idea. \n",
    "\n",
    "Look how big the confidence intervals are. As time goes on, the confidence interval increases since there are more places the real observations could be as it has more time to move.\n",
    "\n",
    "Now so far we have merely tried to predict the companies business using its own past sales. Let us see how well it fares when we implement some hockey data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Hockey data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now let's take a look at the hockey data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hockey_data=pd.read_csv('mtl_hockey_granular.csv',usecols=[\"DATE\",\"AWAY\",\"HOME\"],index_col=\"DATE\",parse_dates=True)\n",
    "hockey_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Just looking at hockey games, what are a few of its characteristics that could determine how much business sports bars get? \n",
    "\n",
    "Is the team playing at home or away? (If at home, they might just go to the stadium instead of the bar)\n",
    "\n",
    "Is the game on a Thursday-Saturday night? Or is it on a monday? \n",
    "\n",
    "Have the MTL Canadiens been losing for a while? Winning for a while? \n",
    "\n",
    "What about the number of goals? Is the variance of goals a big draw?\n",
    "\n",
    "Let's transform the table into a more useful format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hockey_data.columns=['Away','Home']\n",
    "hockey_data[\"MtlGoals\"]=hockey_data.Home\n",
    "hockey_data[\"OppGoals\"]=hockey_data.Home\n",
    "\n",
    "#separating the goals from the name\n",
    "away_goals=hockey_data.Away.str.extract('(\\d+)').astype(int)\n",
    "home_goals=hockey_data.Home.str.extract('(\\d+)').astype(int)\n",
    "\n",
    "#this version will cause the 'A value is trying to be set on a copy of a slice from a DataFrame.' problem\n",
    "#hockey_data[hockey_data.Away.str.contains(\"MTL\")].\"MtlGoals\"=away_goals[hockey_data.Away.str.contains(\"MTL\")].values\n",
    "\n",
    "mtl_home=hockey_data.Home.str.contains(\"MTL\")\n",
    "mtl_away=~hockey_data.Home.str.contains(\"MTL\")\n",
    "\n",
    "hockey_data.loc[mtl_away,\"MtlGoals\"]=away_goals[mtl_away].values\n",
    "hockey_data.loc[mtl_home,\"MtlGoals\"]=home_goals[mtl_home].values\n",
    "\n",
    "hockey_data.loc[mtl_home,\"OppGoals\"]=away_goals[mtl_home].values\n",
    "hockey_data.loc[mtl_away,\"OppGoals\"]=home_goals[mtl_away].values\n",
    "\n",
    "hockey_data.Away=hockey_data.Away.str.replace('\\d+', '')\n",
    "hockey_data.Home=hockey_data.Home.str.replace('\\d+', '')\n",
    "\n",
    "hockey_data[\"Opp\"]=hockey_data.Away\n",
    "hockey_data.loc[mtl_away,\"Opp\"]=hockey_data.Home[mtl_away].values\n",
    "hockey_data.loc[mtl_home,\"Opp\"]=hockey_data.Away[mtl_home].values\n",
    "\n",
    "#I need these to be numbers because later on I will be summing them up\n",
    "\n",
    "hockey_data.loc[mtl_away,\"Away\"]=1\n",
    "hockey_data.loc[mtl_away,\"Home\"]=0\n",
    "\n",
    "hockey_data.loc[mtl_home,\"Away\"]=0\n",
    "hockey_data.loc[mtl_home,\"Home\"]=1\n",
    "\n",
    "hockey_data[\"Win\"]=0\n",
    "hockey_data[\"Tie\"]=0\n",
    "hockey_data[\"Defeat\"]=0\n",
    "\n",
    "\n",
    "wins=hockey_data.MtlGoals>hockey_data.OppGoals\n",
    "ties=hockey_data.MtlGoals==hockey_data.OppGoals\n",
    "losses=hockey_data.MtlGoals<hockey_data.OppGoals\n",
    "\n",
    "hockey_data.loc[wins,\"Win\"]=1\n",
    "hockey_data.loc[ties,\"Tie\"]=1\n",
    "hockey_data.loc[losses,\"Defeat\"]=1\n",
    "\n",
    "#days of the week\n",
    "\n",
    "hockey_data[\"monday\"]=hockey_data.index.dayofweek==0\n",
    "hockey_data[\"tuesday\"]=hockey_data.index.dayofweek==1\n",
    "hockey_data[\"wednesday\"]=hockey_data.index.dayofweek==2\n",
    "hockey_data[\"thursday\"]=hockey_data.index.dayofweek==3\n",
    "hockey_data[\"friday\"]=hockey_data.index.dayofweek==4\n",
    "hockey_data[\"saturday\"]=hockey_data.index.dayofweek==5\n",
    "hockey_data[\"sunday\"]=hockey_data.index.dayofweek==6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hockey_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Much more useful no? We've turned Away and Home into columns holding either 1 or 0 (all from the perspective of the MTL Canadiens of course) and we've added a few more parameters as well. \n",
    "\n",
    "Let us make sure they're all interpreted as numbers (sometimes when you read them from datasources, they're interpreted as \"strings\" or \"objects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hockey_data.Away=pd.to_numeric(hockey_data.Away);\n",
    "hockey_data.Home=pd.to_numeric(hockey_data.Home);\n",
    "hockey_data.MtlGoals=pd.to_numeric(hockey_data.MtlGoals);\n",
    "hockey_data.OppGoals=pd.to_numeric(hockey_data.OppGoals);\n",
    "hockey_data=hockey_data.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For the sports bar data, the most granular I could get it was on a quarterly basis. \n",
    "This means that we need to sum up the hockey activity on a quarterly basis as well to see how one affects the other.\n",
    "The quarters in question are February, May, August, November "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "first_feb=hockey_data.index.month==2\n",
    "index_of_first_feb = np.where(first_feb==True)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The above looks a little hackey, but really it just finds the first occurence of February so that we can start there and then start summing up the data 3 months at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hockey_data=hockey_data.iloc[index_of_first_feb:,:]\n",
    "\n",
    "hockey_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "quarterly_hockey_data=hockey_data.resample(\"3M\").sum(); \n",
    "#turning NaN values into 0s. \n",
    "quarterly_hockey_data=quarterly_hockey_data.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "When we resample quarterly, the pandas dataframe will actually create any datapoints that are missing. In those cases it will produce a row with \"NaN\" so we turn those into 0s. This will be the case for the year 2005 where a hockey lockout occured.\n",
    "\n",
    "It's important that we don't get rid of these empty values because later on will be comparing this with the business data and those dates need to match up, whether hockey occured or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "And there you go, now we have something that matches the quarterly revenue data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "quarterly_hockey_data[-10:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# creating stats again...\n",
    "Just like before, we will be creating statistics based on each one of these columns in the hopes that they can help us with the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#quarterly_hockey_data_extra=create_stats(quarterly_hockey_data[[\"MtlGoals\",\"OppGoals\",\"Win\",\"Tie\",\"Defeat\"]],window=window)\n",
    "quarterly_hockey_data_extra=create_stats(quarterly_hockey_data,window=window,rolling_mean_shifted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "quarterly_hockey_data_with_stats=pd.concat([quarterly_hockey_data,quarterly_hockey_data_extra],axis=1)\n",
    "quarterly_hockey_data_with_stats.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Saving the data so far for future reference..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "quarterly_hockey_data_with_stats.to_csv(\"hockey_data_granular_refined.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "And now we merge the hockey dataset with the sports bar dataset. The following bit of code ensure we deal only with the timespans the hockey and the sports bar dataset overlap on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "dates_in_common=np.intersect1d(quarterly_hockey_data_with_stats.index.values, bar_data.index.values)\n",
    "\n",
    "smaller_hockey_df=quarterly_hockey_data_with_stats.loc[dates_in_common,:]\n",
    "smaller_business_data=bar_data.loc[dates_in_common]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "merged_df=pd.concat([smaller_hockey_df,smaller_business_data],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let us take a quick look at any correlations. While correlation doesn't mean causation, it could give us some clues as to what possible relationships might exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "sns.heatmap(merged_df.corr().abs())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's zoom in on the part we're interested in. Correlation with revenue or any of its variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "correlations_of_interest=merged_df.corr().abs().iloc[-8:,:-8]\n",
    "\n",
    "sns.heatmap(correlations_of_interest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Still too messy. Let's just look at the top candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "min_corr=0.5\n",
    "\n",
    "dict_of_df={}\n",
    "for each_index in correlations_of_interest.index:\n",
    "    cols_of_interest=correlations_of_interest.loc[each_index,:].values>min_corr\n",
    "    if correlations_of_interest.loc[each_index,cols_of_interest].shape[0]>0:\n",
    "        \n",
    "        dict_of_df[each_index]=pd.DataFrame()\n",
    "        dict_of_df[each_index]=correlations_of_interest.loc[each_index,cols_of_interest]\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for each_key in dict_of_df.keys():\n",
    "    print(each_key)\n",
    "    print(dict_of_df[each_key].drop_duplicates().sort_values(ascending=False)[:10])\n",
    "    dict_of_df[each_key].sort_values(ascending=False)[:10].plot(label=each_key,kind=\"bar\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here we can see the correlations of interest and some of it is surprising. Revenue correlating with the number of ties? Or the revenue Relative average rolling mean being correlated with the standard deviation of the number of games on saturdays? \n",
    "\n",
    "Clearly some of these correlations might be spurious. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# TRYING SARIMAX AGAIN, WITH BOTH HOCKEY AND BAR PERFORMANCE\n",
    "\n",
    "Now doing multiple regressions can be tricky. Given some variables it might detect only a few to be statistically significant, yet when you rerun the regression with only those pointed out as statistically significant, their statistical significance might disappear. Let me show what it looks like if we put all of hockey data in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define the p, d and q paradmeters to take any value between 0 and 2\n",
    "p = d = q = range(0, 2)\n",
    "\n",
    "# Generate all different combinations of p, q and q triplets\n",
    "pdq = list(itertools.product(p, d, q))\n",
    "\n",
    "# Generate all different combinations of seasonal p, q and q triplets\n",
    "seasonal_pdq = [(x[0], x[1], x[2], 4) for x in list(itertools.product(p, d, q))]\n",
    "\n",
    "print('Examples of parameter combinations for Seasonal ARIMA...')\n",
    "print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Like before you can set do_sarimax=True if you want to run the grid search, but it could take a while. By default I set it to the answer the gridsearch would have provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "do_sarimax=False\n",
    "x=smaller_hockey_df\n",
    "if do_sarimax==True: #Sarimax can be time consuming. If I already know the answer, don't want this to run again.\n",
    "    warnings.filterwarnings(\"ignore\") # specify to ignore warning messages\n",
    "    best_score, best_cfg = float(\"inf\"), None\n",
    "\n",
    "    \n",
    "\n",
    "    compare_matching_measure_only=False\n",
    "    for each_measure_unit in ['revenueRelAvg','revenueRollingStandardization','revenueRelAvgRollingMean','revenueRollingNormalization']:\n",
    "        print(\"Now checking out \"+each_measure_unit)\n",
    "\n",
    "        chosen_bar_data=smaller_business_data[each_measure_unit]\n",
    "        #COMPARE ONLY WITH MATCHING MEASURE TYPE FOR X\n",
    "        if compare_matching_measure_only==False:\n",
    "\n",
    "            measure_units=['RelAvg','RollingStandardization','RollingNormalization','RelAvgRollingMean']\n",
    "            chosen_measure_unit=[measure_unit for measure_unit in measure_units if measure_unit in each_measure_unit][0]\n",
    "\n",
    "            print(\"chosen_measure_unit is \"+chosen_measure_unit)\n",
    "            cols_I_want=[col for col in x.columns if chosen_measure_unit in col]\n",
    "            x_columns=x[cols_I_want]\n",
    "        else:\n",
    "            x_columns=x;\n",
    "\n",
    "        for param in pdq:\n",
    "            for param_seasonal in seasonal_pdq:\n",
    "\n",
    "                try:\n",
    "                    mod = sm.tsa.statespace.SARIMAX(chosen_bar_data,\n",
    "                                                    order=param,\n",
    "                                                    exog=x_columns,\n",
    "                                                    seasonal_order=param_seasonal,\n",
    "                                                    enforce_stationarity=False,\n",
    "                                                    enforce_invertibility=False)\n",
    "                    results = mod.fit()\n",
    "\n",
    "\n",
    "                    if results.aic< best_score:\n",
    "                        best_score=results.aic\n",
    "                        best_cfg=[param, param_seasonal]\n",
    "                        best_measure_unit=each_measure_unit.copy()\n",
    "                        best_x_columns=x_columns\n",
    "                        best_y_columns=chosen_bar_data\n",
    "\n",
    "                        print('ARIMA{}x{} - AIC:{} with data {}'.format(param, param_seasonal, results.aic,each_measure_unit))\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "    print(\"BEST FIT\")            \n",
    "    print('ARIMA{}x{} - AIC:{} with data {}'.format(best_cfg[0], best_cfg[1], best_score,best_measure_unit))\n",
    "else:\n",
    "    best_measure_unit=\"revenueRelAvg\"\n",
    "    \n",
    "    cols_I_want=[col for col in x.columns if \"RelAvg\" in col]\n",
    "    x_columns=x[cols_I_want]\n",
    "    best_x_columns=x_columns\n",
    "    best_y_columns=smaller_business_data[best_measure_unit]\n",
    "    best_cfg=[(1, 1, 0), (0, 0, 0, 4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "chosen_bar_data=best_y_columns\n",
    "\n",
    "\n",
    "mod = sm.tsa.statespace.SARIMAX(chosen_bar_data,\n",
    "                                order=best_cfg[0],#order=(1, 0, 1),\n",
    "                                exog =best_x_columns, #putting in other regressors\n",
    "                                seasonal_order=best_cfg[1],#seasonal_order=(1, 0, 1, 12),\n",
    "                                enforce_stationarity=False,\n",
    "                                enforce_invertibility=False)\n",
    "\n",
    "results = mod.fit()\n",
    "\n",
    "print(results.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Some analysis...\n",
    "\n",
    "Having put in all hockey variables in one shot through the SARIMAX process, you'll notice that for a lot of them the P value is quite high. \n",
    "\n",
    "Still let's complete the rest of the analysis and see what it would have predicted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "results.plot_diagnostics(figsize=(15, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pred = results.get_prediction(start=pd.to_datetime(y_test.index[0]), dynamic=False)\n",
    "#The dynamic=False argument ensures that we produce one-step ahead forecasts, meaning that forecasts at each point are generated using the full history up to that point.\n",
    "pred_ci = pred.conf_int()\n",
    "\n",
    "ax = chosen_bar_data[mid_data_index:].plot(label='observed')\n",
    "pred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7)\n",
    "\n",
    "ax.fill_between(pred_ci.index,\n",
    "                pred_ci.iloc[:, 0],\n",
    "                pred_ci.iloc[:, 1], color='k', alpha=.2)\n",
    "\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Bar Revenue')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_forecasted = pred.predicted_mean #you'd add whatever other coefficients you want here. \n",
    "y_truth = chosen_bar_data[train_test_index:]\n",
    "\n",
    "# Compute the mean square error\n",
    "mse = ((y_forecasted - y_truth) ** 2).mean()\n",
    "print('The Mean Squared Error of our forecasts is {}'.format(mse))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Again don't be fooled, just because the mean squared error went down from 0.01 to 0.004 doesn't mean it could really predict with greater accuracy on future data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Dynamic forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pred_dynamic = results.get_prediction(start=pd.to_datetime(y_test.index[0]), dynamic=True, full_results=True)\n",
    "pred_dynamic_ci = pred_dynamic.conf_int()\n",
    "\n",
    "ax = chosen_bar_data[mid_data_index:].plot(label='observed', figsize=(20, 15))\n",
    "pred_dynamic.predicted_mean.plot(label='Dynamic Forecast', ax=ax)\n",
    "\n",
    "ax.fill_between(pred_dynamic_ci.index,\n",
    "                pred_dynamic_ci.iloc[:, 0],\n",
    "                pred_dynamic_ci.iloc[:, 1], color='k', alpha=.25)\n",
    "\n",
    "ax.fill_betweenx(ax.get_ylim(), pd.to_datetime(mid_data_index), bar_data.index[-1],\n",
    "                 alpha=.1, zorder=-1)\n",
    "\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Bar performance')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# So where do we go from here? \n",
    "Simply taking out the high P values won't necessarily help because by taking out one variable, the p values of the other parameters change as well. \n",
    "\n",
    "I've decided to use a machine learning technique called ElasticNet Regression to shed some light on what variables (if any) help us in our predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Elastic what?\n",
    "\n",
    "Elastic regression is a combination of a ridge regression and a lasso regression.\n",
    "\n",
    "# Ridge?\n",
    "\n",
    "In a nutshell standard ordinary least squares (OLS) regressions don't typically do well with high dimensional problems.\n",
    "This is because OLS gives the smallest mean squared error among linear estimators with NO BIAS.\n",
    "Ridge regression tries to lower the error even more by adding more of a bias. \n",
    "\n",
    "# Lasso?\n",
    "\n",
    "Lasso is an acronym for \"Least Absolute Selection and Shrinkage Operator\"\n",
    "The only difference between lasso and ridge is that Lasso use what's called the L2 penalty and Ridge uses the L1 penalty. \n",
    "I can talk about the difference in a future post, but to be brief, the lasso can eliminate parameters that aren't statistically significant. Whereas the ridge regression simply keeps lowerering their coefficients but never turning them to 0. \n",
    "\n",
    "\n",
    "# And now the ElasticNet\n",
    "\n",
    "It's a combination of ridge and the lasso. The reasoning is that when you're dealing with lots of variables, you'll often get a lot of them which are correlated with each other and form a sort of cluster if you will. Often times we will want these entire clusters of parameters to remain together should you choose any one of them. If you include one of the parameters you should include them all, if you delete one, you should delete them all. \n",
    "\n",
    "You can see it as the elimination properties of the lasso combined with the more lax \"reduce but don't eliminate\" method of the ridge regression. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Testing for stationarity...\n",
    "\n",
    "While we've already tested for stationarity the series revenueRelAvg, I like to keep this part in as part of my workflow. \n",
    "\n",
    "This allows us or you to try another series if you're curious. You can even difference or log your series if you want and see how that affects things.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#chosen_bar_data=smaller_business_data[\"revenueRelAvg\"]\n",
    "\n",
    "differenced=False\n",
    "logged=False\n",
    "\n",
    "    \n",
    "#if logged:\n",
    "#    chosen_bar_data=np.log1p(chosen_bar_data)\n",
    "\n",
    "    \n",
    "if differenced:\n",
    "    differenced_y=chosen_bar_data-chosen_bar_data.shift()\n",
    "    differenced_y.dropna(inplace=True)\n",
    "\n",
    "else:\n",
    "    differenced_y=chosen_bar_data\n",
    "\n",
    "    \n",
    "if logged:\n",
    "    chosen_bar_data=np.log1p(chosen_bar_data)\n",
    "\n",
    "test_stationarity(differenced_y)\n",
    "\n",
    "\n",
    "#elasticNet parameters\n",
    "l1_ratio=0.5\n",
    "alphas = np.arange(0.0, 3, 0.05)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "Now we'll try training our ElasticNet just using the hockey data, no lags just to see if anything pops up.\n",
    "\n",
    "Here we split up the hockey data into its training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNetCV,ElasticNet #if you want lasso, just put l1_ratio=1 in ElasticNet Config\n",
    "#Constant that multiplies the penalty terms alpha of 0 is more or less equiavlent to doing OLS\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "train_pct=0.8\n",
    "test_pct=1-train_pct\n",
    "\n",
    "x=smaller_hockey_df.loc[differenced_y.index]\n",
    "\n",
    "data_dict=train_test(train_pct,x=x,y=differenced_y)\n",
    "x_train=data_dict[\"x_train\"]\n",
    "x_test=data_dict[\"x_test\"]\n",
    "y_train=data_dict[\"y_train\"]\n",
    "y_test=data_dict[\"y_test\"]\n",
    "mid_data_index=data_dict[\"mid_data_index\"]\n",
    "all_y=differenced_y\n",
    "all_x=x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The following function was taken from https://www.kaggle.com/apapiu/regularized-linear-models. It allows us to see how much regularization is best for us. You can configure how much regularizarion you want through the alpha variable\n",
    "\n",
    "Regularization is basically a trade off between overfitting too much (where you infer too much from your training data) and over generalizing (the most extreme being just a straight line)\n",
    "\n",
    "We set this through cross-validation which splits up your dataset into different train and test batches to see how well your model would have done had the training and testing sets been different. \n",
    "\n",
    "In our case, because we're dealing with a time series, we will be using a special type of cross-validation that maintains the order in time of the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def rmse_cv(model,x_train, y_train,cv=5):\n",
    "    #neg_mean_squared_error, neg_mean_absolute_error\n",
    "    rmse= np.sqrt(-cross_val_score(model, x_train, y_train, scoring=\"neg_mean_squared_error\", cv = 5))\n",
    "\t#with scoring being blank, by default this would've outputted the accuracy, ex: 95%\n",
    "\t#with scoring=\"neg_mean_squared_error\", we get accuracy -1, so shows by how much you were off and it's negative\n",
    "\t#then with the - in front, gives you the error, but positive. \n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "cv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "cv_elastic = [rmse_cv(ElasticNet(normalize =False,alpha = alpha,l1_ratio=l1_ratio,max_iter=1000),x_train, y_train,cv=cv).mean() for alpha in alphas]\n",
    "\n",
    "cv_elastic = pd.Series(cv_elastic, index = alphas)\n",
    "cv_elastic.plot(title = \"Validation\")\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"rmse\")\n",
    "\n",
    "chosen_alpha=cv_elastic.idxmin() #where the error is lowest\n",
    "\n",
    "print(\"alpha chosen at %f\" %chosen_alpha)\n",
    "\n",
    "model_elastic=ElasticNet(normalize =False,alpha = chosen_alpha,l1_ratio=l1_ratio,max_iter=1000).fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "#FEATURE SELECTION\n",
    "#getting an idea of what's important and what isn't.  \n",
    "\n",
    "features=x_train.columns\n",
    "coef = pd.Series(model_elastic.coef_, index = features)\n",
    "nber_of_variables=sum(coef != 0)\n",
    "\n",
    "print(\"Lasso picked \" + str(nber_of_variables) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")\n",
    "\n",
    "print(\"score of %f\"%model_elastic.score(x_train,y_train))\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "coef.abs().sort_values()[-nber_of_variables:].plot(kind = \"barh\")\n",
    "\n",
    "plt.title(\"Coefficients in the Lasso Model\")\n",
    "\n",
    "#let's say we keep \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Well, it looks like the elastic net picked up only variable that was statistically significant on its own. But even at that, it seems very weak, just look at that coefficient..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Plotting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "pred=pd.DataFrame(model_elastic.predict(x),index=x.index)\n",
    "\n",
    "answer=pd.concat([pred,differenced_y],axis=1)\n",
    "\n",
    "answer.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#This is to put it back into its original measurement.\n",
    "def undo_differencing(original_y,prediction):\n",
    "    original_y=pd.DataFrame(original_y)\n",
    "    prediction=pd.DataFrame(prediction)\n",
    "    answer = pd.Series(original_y.iloc[0].values, index=original_y.index)   \n",
    "\n",
    "    combined=pd.concat([answer,prediction.cumsum()],axis=1).fillna(0)\n",
    "\n",
    "    combined=combined.sum(axis=1)\n",
    "    return combined\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In case IF I had differend the series or logged it, I would undo them here, so as to see what our final prediction would have been. Also here we would've had a step forward prediction, whereby we'd be using the a real observation at each step to help predict the next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#STEP FORWARD PREDICTION\n",
    "\n",
    "prediction=model_elastic.predict(x)\n",
    "prediction=pd.Series(prediction,index=x.index)\n",
    "\n",
    "\n",
    "if logged:\n",
    "    prediction=np.exp(prediction)-1\n",
    "\n",
    "if differenced:\n",
    "    final_pred=(prediction+chosen_bar_data.shift()).dropna()\n",
    "\n",
    "else:    \n",
    "    #WITHOUT DIFFERENCING\n",
    "    final_pred=pd.Series(prediction,index=x.index)\n",
    "    #final_pred=np.exp(final_pred)\n",
    "\n",
    "    \n",
    "answer=pd.concat([final_pred,chosen_bar_data],axis=1,ignore_index=True)\n",
    "\n",
    "#answer\n",
    "\n",
    "forecast_mse = ((final_pred - chosen_bar_data) ** 2).mean()\n",
    "baseline_mse = ((chosen_bar_data-chosen_bar_data.shift()) ** 2).mean()\n",
    "no_guess_mse=((chosen_bar_data) ** 2).mean()\n",
    "\n",
    "print(\"My forecast MSE : {}, \\nsimply using previous value MSE : {}, \\nstraight line MSE :{}\".format(forecast_mse,baseline_mse,no_guess_mse))\n",
    "\n",
    "\n",
    "answer.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As you can see, based on the montreal goals, it is only willing to predict so much one way or another. Still it is interesting to note that it is performing better than simply a straight line through 0 as well as simply predicting the previous value, so something is going on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#DYNAMIC PREDICTION\n",
    "if logged:\n",
    "    prediction=np.exp(prediction)-1\n",
    "\n",
    "    \n",
    "if differenced:\n",
    "    final_pred=undo_differencing(chosen_bar_data,pd.Series(prediction,index=x.index))\n",
    "\n",
    "else:    \n",
    "    #WITHOUT DIFFERENCING\n",
    "    final_pred=pd.Series(prediction,index=x.index)\n",
    "    #final_pred=np.exp(final_pred)\n",
    "\n",
    "answer=pd.concat([final_pred,chosen_bar_data],axis=1,ignore_index=True)\n",
    "\n",
    "#answer\n",
    "\n",
    "forecast_mse = ((final_pred - chosen_bar_data) ** 2).mean()\n",
    "\n",
    "baseline_mse = ((chosen_bar_data-chosen_bar_data.shift()) ** 2).mean()\n",
    "\n",
    "no_guess_mse=((chosen_bar_data) ** 2).mean()\n",
    "\n",
    "print(\"My forecast MSE : {}, \\nsimply using previous value MSE : {}, \\nstraight line MSE :{}\".format(forecast_mse,baseline_mse,no_guess_mse))\n",
    "\n",
    "answer.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "If we were dealing with differenced data, above we'd have the dynamic prediction whereby we predict the next value based on our previously predicted values for the current period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# ElasticNet With Lags\n",
    "\n",
    "Out of curiosity let's just see what the ElasticNet thinks of using past values to help with the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#list_of_lags=list(range(1,5)\n",
    "def make_lags(list_of_lags,y_serie):\n",
    "    data=pd.DataFrame()\n",
    "    for each in list_of_lags:\n",
    "        lag=y_serie.shift(each)\n",
    "        lag=lag.rename(\"lag %i\"%each)\n",
    "        data=pd.concat([data,lag],axis=1)\n",
    "    data.dropna(inplace=True)\n",
    "    return data\n",
    "\n",
    "y_train_lags=make_lags(list(range(1,5)),y_train)\n",
    "y_test_lags=make_lags(list(range(1,5)),y_test)\n",
    "all_y_lags=make_lags(list(range(1,5)),all_y)\n",
    "\n",
    "x_train_with_lags=pd.concat([x_train,y_train_lags],axis=1)\n",
    "x_train_with_lags.dropna(inplace=True)\n",
    "x_test_with_lags=pd.concat([x_test,y_test_lags],axis=1)\n",
    "x_test_with_lags.dropna(inplace=True)\n",
    "all_x_with_lags=pd.concat([x_train,y_train_lags],axis=1)\n",
    "all_x_with_lags.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#because of the lags, you can't compare with all of y_train, the lags will lower the number of data points for your input\n",
    "dates_in_common_train=np.intersect1d(x_train_with_lags.index.values, y_train.index.values)\n",
    "y_train_lagged=y_train.loc[dates_in_common_train]\n",
    "\n",
    "dates_in_common_test=np.intersect1d(x_test_with_lags.index.values, y_test.index.values)\n",
    "y_test_lagged=y_test.loc[dates_in_common_test]\n",
    "\n",
    "dates_in_common_all=np.intersect1d(all_x_with_lags.index.values, all_y.index.values)\n",
    "all_y_lagged=all_y.loc[dates_in_common_all]\n",
    "\n",
    "chosen_bar_data_lagged=chosen_bar_data.loc[dates_in_common_all]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "cv_elastic = [rmse_cv(ElasticNet(normalize =False,alpha = alpha,l1_ratio=l1_ratio,max_iter=1000),x_train_with_lags, y_train_lagged,cv=cv).mean() for alpha in alphas]\n",
    "\n",
    "cv_elastic = pd.Series(cv_elastic, index = alphas)\n",
    "cv_elastic.plot(title = \"Validation\")\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"rmse\")\n",
    "\n",
    "chosen_alpha=cv_elastic.idxmin() #where the error is lowest\n",
    "\n",
    "print(\"alpha chosen at %f\" %chosen_alpha)\n",
    "\n",
    "\n",
    "model_elastic=ElasticNet(normalize =False,alpha = chosen_alpha,l1_ratio=l1_ratio,max_iter=1000).fit(x_train_with_lags, y_train_lagged)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#FEATURE SELECTION\n",
    "#getting an idea of what's important and what isn't.  \n",
    "features=x_train_with_lags.columns\n",
    "coef = pd.Series(model_elastic.coef_, index = features)\n",
    "nber_of_variables=sum(coef != 0)\n",
    "\n",
    "print(\"Lasso picked \" + str(nber_of_variables) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")\n",
    "\n",
    "print(\"score of %f\"%model_elastic.score(x_train_with_lags,y_train_lagged))\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "coef.abs().sort_values()[-nber_of_variables:].plot(kind = \"barh\")\n",
    "\n",
    "\n",
    "if nber_of_variables==0:\n",
    "    winning_coef=[\"\"]\n",
    "else:\n",
    "    winning_coef=coef.abs().sort_values()[-nber_of_variables:].index.values\n",
    "    winning_coef=winning_coef[::-1]\n",
    "\n",
    "\n",
    "plt.title(\"Coefficients in the Lasso Model\")\n",
    "\n",
    "#let's say we keep \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "No lags were kept! Even lags 2 and 4 didn't make it. Interesting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#STEP FORWARD PREDICTION\n",
    "\n",
    "prediction=model_elastic.predict(all_x_with_lags)\n",
    "prediction=pd.Series(prediction,index=all_x_with_lags.index)\n",
    "\n",
    "if logged:\n",
    "    prediction=np.exp(prediction)-1\n",
    "\n",
    "if differenced:\n",
    "    final_pred=(prediction+chosen_bar_data_lagged.shift()).dropna()\n",
    "\n",
    "else:    \n",
    "    #WITHOUT DIFFERENCING\n",
    "    final_pred=pd.Series(prediction,index=all_x_with_lags.index)\n",
    "    #final_pred=np.exp(final_pred)\n",
    "\n",
    "    \n",
    "answer=pd.concat([final_pred,chosen_bar_data_lagged],axis=1,ignore_index=True)\n",
    "\n",
    "\n",
    "#answer\n",
    "\n",
    "forecast_mse = ((final_pred - chosen_bar_data_lagged) ** 2).mean()\n",
    "baseline_mse = ((all_y_lagged-chosen_bar_data_lagged.shift()) ** 2).mean()\n",
    "no_guess_mse=((chosen_bar_data_lagged) ** 2).mean()\n",
    "\n",
    "print(\"My forecast MSE : {}, \\nsimply using previous value MSE : {}, \\nstraight line MSE :{}\".format(forecast_mse,baseline_mse,no_guess_mse))\n",
    "\n",
    "\n",
    "answer.plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#DYNAMIC PREDICTION\n",
    "\n",
    "prediction=model_elastic.predict(all_x_with_lags)\n",
    "prediction=pd.Series(prediction,index=all_x_with_lags.index)\n",
    "\n",
    "if logged:\n",
    "    prediction=np.exp(prediction)-1\n",
    "\n",
    "    \n",
    "if differenced:\n",
    "    final_pred=undo_differencing(chosen_bar_data_lagged,pd.Series(prediction,index=all_x_with_lags.index))\n",
    "\n",
    "else:    \n",
    "    #WITHOUT DIFFERENCING\n",
    "    final_pred=pd.Series(prediction,index=all_x_with_lags.index)\n",
    "    #final_pred=np.exp(final_pred)\n",
    "\n",
    "    \n",
    "\n",
    "answer=pd.concat([final_pred,chosen_bar_data_lagged],axis=1).dropna()\n",
    "\n",
    "#answer\n",
    "forecast_mse = ((final_pred - chosen_bar_data_lagged) ** 2).mean()\n",
    "baseline_mse = ((chosen_bar_data_lagged-chosen_bar_data_lagged.shift()) ** 2).mean()\n",
    "no_guess_mse=((chosen_bar_data_lagged) ** 2).mean()\n",
    "\n",
    "print(\"My forecast MSE : {}, \\nsimply using previous value MSE : {}, \\nstraight line MSE :{}\".format(forecast_mse,baseline_mse,no_guess_mse))\n",
    "\n",
    "answer.plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Back to Sarimax\n",
    "\n",
    "So we've got MtlGoals as the only significant variable, which kind of makes sense. \n",
    "\n",
    "It is a little weird that none of the lags made it, however. \n",
    "\n",
    "Let's see if adding MtlGoals to our Sarimax model improves things. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define the p, d and q paradmeters to take any value between 0 and 2\n",
    "p =  q = range(0, 4)\n",
    "d =range(0, 2)\n",
    "\n",
    "# Generate all different combinations of p, q and q triplets\n",
    "pdq = list(itertools.product(p, d, q))\n",
    "\n",
    "\n",
    "# Generate all different combinations of seasonal p, q and q triplets\n",
    "seasonal_pdq = [(x[0], x[1], x[2], 2) for x in list(itertools.product(p, d, q))]\n",
    "\n",
    "print('Examples of parameter combinations for Seasonal ARIMA...')\n",
    "print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Like before you can run the gridsearch if you want, but the answer has been saved in the next cell. It's \n",
    "\n",
    "BEST FIT\n",
    "ARIMA(0, 0, 0)x(1, 0, 0, 2) - AIC:-74.1056787276343\n",
    "parameters in play\n",
    "['MtlGoals']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "do_sarimax=False\n",
    "if do_sarimax==True:    \n",
    "    #cols_I_want=winning_coef\n",
    "    cols_I_want=[\"MtlGoals\"]  \n",
    "    cols_in_play=[]\n",
    "    best_score, best_cfg = float(\"inf\"), None\n",
    "    for each_col in cols_I_want:\n",
    "        cols_in_play.append(each_col)\n",
    "        print(each_col)\n",
    "        x_columns=x_train[cols_in_play]\n",
    "\n",
    "        warnings.filterwarnings(\"ignore\") # specify to ignore warning messages\n",
    "\n",
    "        for param in pdq:\n",
    "            for param_seasonal in seasonal_pdq:\n",
    "\n",
    "                try:\n",
    "                    mod = sm.tsa.statespace.SARIMAX(y_train,\n",
    "                                            order=param,#order=(1, 0, 1),\n",
    "                                            exog =x_columns, #putting in other regressors\n",
    "                                            seasonal_order=param_seasonal,#seasonal_order=(1, 0, 1, 12),\n",
    "                                            enforce_stationarity=False,\n",
    "                                            enforce_invertibility=False)\n",
    "\n",
    "                    results = mod.fit()\n",
    "\n",
    "\n",
    "                    if results.aic< best_score:\n",
    "                        best_score=results.aic\n",
    "                        best_cfg=[param, param_seasonal]\n",
    "\n",
    "                        parameters_in_play=cols_in_play.copy()\n",
    "                        print('ARIMA{}x{} - AIC:{}'.format(param, param_seasonal, results.aic))\n",
    "                        #print(\"best parameters so far\")\n",
    "                        #print(parameters_in_play)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "    print(\"BEST FIT\")            \n",
    "    print('ARIMA{}x{} - AIC:{}'.format(best_cfg[0], best_cfg[1], best_score))\n",
    "    print(\"parameters in play\")\n",
    "    print(parameters_in_play)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cols_in_play=[]\n",
    "\n",
    "cols_I_want=[\"MtlGoals\"] #wednesdayRelAvgDifferenced\n",
    "#cols_I_want=parameters_in_play \n",
    "x_columns=all_x[cols_I_want] #TieRollingStd\n",
    "\n",
    "\n",
    "best_cfg=[(0, 0, 0),(1, 0, 0, 2)] #with MtlGoals\n",
    "\n",
    "#with no hockey,\n",
    "#best_cfg=[(0, 0, 0)x(1, 0, 1, 2)]   \n",
    "\n",
    "\n",
    "mod = sm.tsa.statespace.SARIMAX(all_y,\n",
    "                                order=best_cfg[0],#order=(1, 0, 1),\n",
    "                                exog =x_columns, #putting in other regressors\n",
    "                                #exog=None,\n",
    "                                seasonal_order=best_cfg[1],#seasonal_order=(1, 0, 1, 12),\n",
    "                                enforce_stationarity=False,\n",
    "                                enforce_invertibility=False)\n",
    "\n",
    "results = mod.fit()\n",
    "print(results.summary())\n",
    "\n",
    "results.plot_diagnostics(figsize=(15, 12))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"PREDICTION NOT DYNAMIC\")\n",
    "pred = results.get_prediction(start=pd.to_datetime(y_test.index[0]), dynamic=False)\n",
    "#The dynamic=False argument ensures that we produce one-step ahead forecasts, meaning that forecasts at each point are generated using the full history up to that point.\n",
    "pred_ci = pred.conf_int()\n",
    "\n",
    "ax = all_y[mid_data_index:].plot(label='observed')\n",
    "pred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7)\n",
    "\n",
    "ax.fill_between(pred_ci.index,\n",
    "                pred_ci.iloc[:, 0],\n",
    "                pred_ci.iloc[:, 1], color='k', alpha=.2)\n",
    "\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Bar Revenue')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"TEST ERRORS ANALYSIS\")\n",
    "\n",
    "\n",
    "\n",
    "y_forecasted = pred.predicted_mean #you'd add whatever other coefficients you want here. \n",
    "y_truth=y_test_lagged\n",
    "\n",
    "\n",
    "forecast_mse = ((y_forecasted - y_truth) ** 2).mean()\n",
    "\n",
    "baseline_mse = ((y_truth-y_truth.shift()) ** 2).mean()\n",
    "\n",
    "no_guess_mse=((y_truth) ** 2).mean()\n",
    "\n",
    "print(\"My forecast MSE : {}, \\nsimply using previous value MSE : {}, \\nstraight line MSE :{}\".format(forecast_mse,baseline_mse,no_guess_mse))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"PREDICTION DYNAMIC\")\n",
    "pred_dynamic = results.get_prediction(start=pd.to_datetime(y_test.index[0]), dynamic=True, full_results=True)\n",
    "\n",
    "pred_dynamic_ci = pred_dynamic.conf_int()\n",
    "\n",
    "ax = all_y[mid_data_index:].plot(label='observed', figsize=(20, 15))\n",
    "pred_dynamic.predicted_mean.plot(label='Dynamic Forecast', ax=ax)\n",
    "\n",
    "ax.fill_between(pred_dynamic_ci.index,\n",
    "                pred_dynamic_ci.iloc[:, 0],\n",
    "                pred_dynamic_ci.iloc[:, 1], color='k', alpha=.25)\n",
    "\n",
    "ax.fill_betweenx(ax.get_ylim(), pd.to_datetime(mid_data_index), bar_data.index[-1],\n",
    "                 alpha=.1, zorder=-1)\n",
    "\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Bar performance')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"TEST ERRORS ANALYSIS\")\n",
    "\n",
    "y_forecasted = pred_dynamic.predicted_mean\n",
    "#y_truth = other_y_test #from the test going forward only\n",
    "\n",
    "forecast_mse = ((y_forecasted - y_truth) ** 2).mean()\n",
    "baseline_mse = ((y_truth-y_truth.shift()) ** 2).mean()\n",
    "no_guess_mse=((y_truth) ** 2).mean()\n",
    "\n",
    "print(\"My forecast MSE : {}, \\nsimply using previous value MSE : {}, \\nstraight line MSE :{}\".format(forecast_mse,baseline_mse,no_guess_mse))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Above you'll see both the regular and dynamic regressions. It looks like the best model including the MtlGoals variable outperforms our previous model that didn't have any hockey data at all. \n",
    "\n",
    "And also look at those P values, they are all below 0.05. And finally the AIC score is lower with a value of -91\n",
    "\n",
    "But after all this, it must noted as well that it didn't beat simpling predicting the value of 0 for this test case. Predicting 0 means that the best guess for the next quarterly revenue is the current quarterly revenue average.\n",
    "\n",
    "But still if you look at the training data, a lot of the time the values were much higher than 0. It just happens that in the test case it was all very close to 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Here are the final results\n",
    "\n",
    "Mean squared error (Not Dynamic)\n",
    "\n",
    "With MtlGoals: 0.00855 --- Without MtlGoals 0.0129\n",
    "\n",
    "Mean squared error (Dynamic) \n",
    "\n",
    "With MtlGoals: 0.00897 --- Without MtlGoals 0.0144\n",
    "\n",
    "AIC score:\n",
    "\n",
    "With MTL Goals: -91 -- Without MtlGoals -64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "And there you have it, when predicting the quarterly revenue of Sportscene relative to its 2 year average, it seems the number of goals scored by the Montreal Canadiens team does help in predicting the next quarterly revenue at least a little bit.\n",
    "\n",
    "This also means that MOST of the sports bar business' success is NOT explained by the performance of the Montreal Canadiens. They could always lose and it wouldn't affect them that much. \n",
    "\n",
    "This either means:\n",
    "\n",
    "Loyal fans will show up no matter what.\n",
    "\n",
    "A large portion of the sports' bars clientel show up for something else than hockey. It could be for other sports, or maybe the lively atmosphere for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Other things to explore\n",
    "\n",
    "There was tremendous growth earlier on in the timeseries. Compared with the flattish quarterly revenue (2009 and onward), this could be seen as a \"paradigm\" shift. We could try running the training only on 2009 onwards to see what it would give.\n",
    "\n",
    "We could try automatically detecting a change in behaviour like this with Monte Carlo Markov Chains.\n",
    "\n",
    "And finally seeing as there are only 4 periods in a year, why could just try turning them into dummy variables and running the analysis as as normal series (with independent datapoints) instead of a timeseries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
